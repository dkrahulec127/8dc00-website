

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Topic 2.3: Generalization and overfitting &mdash; Medical Image Analysis (8DC00) v0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Topic 2.4: Logistic regression" href="2.4_CAD_logistic_regression.html" />
    <link rel="prev" title="Topic 2.2: k-Means, k-Nearest Neighbors, decision trees" href="2.2_CAD_kNN_decision_trees.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Medical Image Analysis (8DC00)
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="0.1_Python_programming_skills.html">Topic 0.1: Course practicalities</a></li>
</ul>
<p class="caption"><span class="caption-text">Registration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="1.1_Registration_geometrical-transformations.html">Topic 1.1: Geometrical transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.2_Registration_point-based-registration.html">Topic 1.2: Point-based registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.3_Registration_intensity-based-registration.html">Topic 1.3: Intensity-based registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.5_Registration_active-shape-models.html">Topic 1.5: Active shape models</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.6_Registration_project.html">Project 1: Medical image registration</a></li>
</ul>
<p class="caption"><span class="caption-text">Computer-aided diagnosis</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="2.1_CAD_linear_regression.html">Topic 2.1: Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.2_CAD_kNN_decision_trees.html">Topic 2.2: k-Means, k-Nearest Neighbors, decision trees</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Topic 2.3: Generalization and overfitting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#1.-Learning-process-of-a-neural-network">1. Learning process of a neural network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#2.-Generalization-and-overfitting">2. Generalization and overfitting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Exercise-2.1.1:"><em>Exercise 2.1.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Exercise-2.1.2:"><em>Exercise 2.1.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.1.1:"><em>Question 2.1.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.1.2:"><em>Question 2.1.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#2.2.-Learning-curves:-pen-and-paper-exercise">2.2. Learning curves: pen and paper exercise</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.2.1:"><em>Question 2.2.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.2.2:"><em>Question 2.2.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Exercise-2.3.1:"><em>Exercise 2.3.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.3.1:"><em>Question 2.3.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.3.2:"><em>Question 2.3.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.3.3:"><em>Question 2.3.3</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Exercise-2.4.1:"><em>Exercise 2.4.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.4.1:"><em>Question 2.4.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.4.2:"><em>Question 2.4.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.5.1:"><em>Question 2.5.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.5.2:"><em>Question 2.5.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.5.3:"><em>Question 2.5.3</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.6.1:"><em>Question 2.6.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.6.2:"><em>Question 2.6.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Exercise-2.6.1:"><em>Exercise 2.6.1</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Exercise-2.6.2:"><em>Exercise 2.6.2</em>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Question-2.6.3:"><em>Question 2.6.3</em>:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2.4_CAD_logistic_regression.html">Topic 2.4: Logistic regression</a></li>
</ul>
<p class="caption"><span class="caption-text">Convolutional neural networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2.5_CNNs_fundamental_building_blocks.html">Topic 2.5: Fundamental building blocks of neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.6_CNNs_unsupervised_learning_PCA.html">Topic 2.6: (Un)supervised learning, PCA</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.8_CAD_project.html">Project 2: Computer-aided diagnosis</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Medical Image Analysis (8DC00)</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Topic 2.3: Generalization and overfitting</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/reader/2.3_CAD_generalization_overfitting.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Topic-2.3:-Generalization-and-overfitting">
<h1>Topic 2.3: Generalization and overfitting<a class="headerlink" href="#Topic-2.3:-Generalization-and-overfitting" title="Permalink to this headline">¶</a></h1>
<p>This notebook combines theory with exercises to support the understanding of generalization and overfitting in neural networks. Implement all functions in the <code class="docutils literal notranslate"><span class="pre">code</span></code> folder of your cloned repository, and test it in this notebook after implementation by importing your functions to this notebook. Use available markdown sections to fill in your answers to questions as you proceed through the notebook.</p>
<p><strong>Contents:</strong></p>
<ol class="arabic simple">
<li><p><a class="reference external" href="#learning_process_nn">Learning process of a neral network</a></p></li>
<li><p><a class="reference external" href="#generalization_overfitting">Generalization and overfitting</a> 2.1 <a class="reference external" href="#learning_curves">Learning curves</a> 2.2 <a class="reference external" href="#pen_paper">Learning curves - pen and paper exercise</a> 2.3 <a class="reference external" href="#feature_curve">Feature curves (OPTIONAL)</a> 2.4 <a class="reference external" href="#distances">Distances in high dimensions (OPTIONAL)</a> 2.5 <a class="reference external" href="#surprising_props">More surprising properties of high dimensions (OPTIONAL)</a> 2.6 <cite>:math:`k</cite>-different features, different samples (OPTIONAL) &lt;#k_different_features&gt;`__</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
</pre></div></div>
</div>
<p><p><img alt="43a09397245b4a938ee0759f2b9a98ce" class="no-scaled-link" src="../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="1.-Learning-process-of-a-neural-network">
<h2>1. Learning process of a neural network<a class="headerlink" href="#1.-Learning-process-of-a-neural-network" title="Permalink to this headline">¶</a></h2>
<p>Before learning about generalization and overfitting, let’s first understand how a neural network learns. As humans we are capable of learnign many tasks throughout our lives. We can for example easily distinguish cats from dogs in a picture, but we were not able to do this as newborns and we had to learn this along the way. In our upbringing, constant feedback is given by parents and teachers to make sure that we can recognize different common animals or objects. So after a while you simply
know what animal you are seeing by taking a quick look at the animal. However, we find it more difficult to recognize rare animals, the reason for this is that we did not see many examples of these rare animals.</p>
<p>This principle is exactly the same for a neural network. During the training process, known data is fed into the neural network, and the network makes a prediction about what the data represents. Any error in the prediction is used as feedback. As the training process continues, the network weights are adjusted (using backpropagation) until the network is making accurate predictions. Then the model is ready and can be used to make predictions for unseen images in the inference stage. This
process is visualized in the figure below. As you can see, the model is only trained on three classes of images (triangles, stars and circles), therefore it will never be able to classify other shapes. However, the model is able to classify a green star as a star even though this exact color star is not seen during training, but simply because the training data consists of a large variety of colors.</p>
<p><img alt="b74f2bf6f40c47a295f32ae39f1006b7" class="no-scaled-link" src="../_images/00_training_inference.jpg" style="width: 800px;" /></p>
<p><p><img alt="6810471bb3214bc2a65c34a187379c00" class="no-scaled-link" src="../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="2.-Generalization-and-overfitting">
<h2>2. Generalization and overfitting<a class="headerlink" href="#2.-Generalization-and-overfitting" title="Permalink to this headline">¶</a></h2>
<p>Generalization and overfitting are crucial terms in machine learning algorithms. Generalization describes a model’s ability to make accurate predictions based on new data previously absent from the training dataset. Model’s generalization capability can be thought of as a success measure in making accurate predictions. However, if a model has been trained too well on training data, it will make inaccurate predictions on new data. The opposite holds as well. Underfitting can happen when a model
has been trained insufficiently.</p>
<p>In practice, three datasets are used in deep learning research: 1. <strong>Training set</strong> - The training set is used for training the model (i.e. iteratively updating the network weights to minimize the error). 2. <strong>Validation set</strong> - At the inference phase, the validation set is used for two purposes: * Check for model overfitting: Sometimes the model is able to ‘remember’ all the training examples, which means that the model will not generalize well to unseen data at the inference phase.
Overfitting can be detected by inspecting the training and validation loss over time. The loss function is often based on the error between the model prediction and the desired output, this means that you want to minimize the loss function. As you can see in the figure below, the training and validation loss show the same pattern when the model is not overfitting (left figure). When the model is overfitting to the training data, you see that the training and validation loss start to diverge
after a certain number of epochs (right figure), this tells you that the model is not generalizing well to new data. <img alt="ba19c5646e3d4052bab55c0adfa66256" class="no-scaled-link" src="../_images/00_overfitting.jpg" style="width: 880px;" /></p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>* Tuning of model parameters: Many parameters (e.g. number of layers, loss function, learning rate, etc.) influence the performance of the model for a specific task. The performance can often be measured with a quantitative metric because the desired output (ground truth) is known for the validation set. By systematically adapting model parameters and evaluating the performance, the optimal parameters can be chosen.
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Test set</strong> - The test set is used to show the final performance of the model on a unseen set. This performance can give an indication of how the model will perform when it is implemented in for example the clinic (with the assumption that the test set resembles the real clinical data in terms of population and acquisition protocol).</p></li>
</ol>
<p>### 2.1. Learning curves</p>
<p>Let’s investigate how different sizes of the training set affect the results. So far you have been using a training set of 100 samples. Are these good values? Maybe we can do with less samples, or we can improve if we add more?</p>
<p><p><img alt="9ee3e23306de4bb7801932899f06ab77" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="Exercise-2.1.1:">
<h3><em>Exercise 2.1.1</em>:<a class="headerlink" href="#Exercise-2.1.1:" title="Permalink to this headline">¶</a></h3>
<p>Use the provided script <code class="docutils literal notranslate"><span class="pre">learning_curve()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">2</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module. Run the script, which will produce a plot of the error against the size of the training set. What training set size would you say is good enough, i.e. the performance does not increase a lot afterwards? What about if you load the brain data instead of the Gaussian datasets?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">learning_curve</span>

<span class="n">learning_curve</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
train_size = 1, iter = 0
train_size = 1, iter = 1
train_size = 1, iter = 2
train_size = 3, iter = 0
train_size = 3, iter = 1
train_size = 3, iter = 2
train_size = 10, iter = 0
train_size = 10, iter = 1
train_size = 10, iter = 2
train_size = 30, iter = 0
train_size = 30, iter = 1
train_size = 30, iter = 2
train_size = 100, iter = 0
train_size = 100, iter = 1
train_size = 100, iter = 2
train_size = 300, iter = 0
train_size = 300, iter = 1
train_size = 300, iter = 2
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/reader_2.3_CAD_generalization_overfitting_5_1.png" src="../_images/reader_2.3_CAD_generalization_overfitting_5_1.png" />
</div>
</div>
<p><p><img alt="4444f2dbca28430dad4a9524b203d9a2" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.1.2:">
<h3><em>Exercise 2.1.2</em>:<a class="headerlink" href="#Exercise-2.1.2:" title="Permalink to this headline">¶</a></h3>
<p>Modify the <code class="docutils literal notranslate"><span class="pre">learning_curve()</span></code> script so that it also plots the training error.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">learning_curve</span>

<span class="n">learning_curve</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
train_size = 1, iter = 0
train_size = 1, iter = 1
train_size = 1, iter = 2
train_size = 3, iter = 0
train_size = 3, iter = 1
train_size = 3, iter = 2
train_size = 10, iter = 0
train_size = 10, iter = 1
train_size = 10, iter = 2
train_size = 30, iter = 0
train_size = 30, iter = 1
train_size = 30, iter = 2
train_size = 100, iter = 0
train_size = 100, iter = 1
train_size = 100, iter = 2
train_size = 300, iter = 0
train_size = 300, iter = 1
train_size = 300, iter = 2
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/reader_2.3_CAD_generalization_overfitting_7_1.png" src="../_images/reader_2.3_CAD_generalization_overfitting_7_1.png" />
</div>
</div>
<p><p><img alt="1e5e8850a9774f7eadefae1b8a0c1f7e" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.1.1:">
<h3><em>Question 2.1.1</em>:<a class="headerlink" href="#Question-2.1.1:" title="Permalink to this headline">¶</a></h3>
<p>From what you learned in class, how do you expect the training error plot to look like for 1-NN classifier?</p>
<p>Type your answer here</p>
<p><p><img alt="1182d2ad223345bd8aaf92208eb0a10d" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.1.2:">
<h3><em>Question 2.1.2</em>:<a class="headerlink" href="#Question-2.1.2:" title="Permalink to this headline">¶</a></h3>
<p>The 1-NN classifier suffers from overfitting: there is a gap between the training error and the test error. Try other values of <span class="math notranslate nohighlight">\(k\)</span> and observe how the learning curves change. What happens?</p>
<p>Type your answer here</p>
<p><p><img alt="9f252a70589f413bb434215f56cab09e" class="no-scaled-link" src="../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="2.2.-Learning-curves:-pen-and-paper-exercise">
<h3>2.2. Learning curves: pen and paper exercise<a class="headerlink" href="#2.2.-Learning-curves:-pen-and-paper-exercise" title="Permalink to this headline">¶</a></h3>
<p>Consider the following figures, which show the so called “banana dataset”, and learning curves A, B and C for this dataset. These learning curves are made by two of the following three classifiers: a nearest mean classifier, a 1-nearest neighbor classifier, and a 5-nearest neighbor classifier.</p>
<p><img alt="078681beea334b529e029f5eefec639f" class="no-scaled-link" src="../_images/banana_grid.png" style="width: 600px; height: 600px;" /></p>
<div style="text-align: center"><p>Figure 2.2.1: top left: dataset, top right: learning curve A, bottom left: learning curve B, bottom right: learning curve C</p>
</div><p><p><img alt="a5d5fcdf01c94f7c96c114d6111d2efe" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.2.1:">
<h3><em>Question 2.2.1</em>:<a class="headerlink" href="#Question-2.2.1:" title="Permalink to this headline">¶</a></h3>
<p>Which learning curve belongs to which classifier? Explain why you think this is the case.</p>
<p>Type your answer here</p>
<p><p><img alt="72ca91e7513a438cab7d80f590c4befa" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.2.2:">
<h3><em>Question 2.2.2</em>:<a class="headerlink" href="#Question-2.2.2:" title="Permalink to this headline">¶</a></h3>
<p>Which learning curve do you think will improve the most if we were to add more training samples? Why?</p>
<p>Type your answer here</p>
<p>### 2.3. Feature curve (OPTIONAL)</p>
<p><p><img alt="bb4a5e6b1f304a44941c30adb9823883" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.3.1:">
<h3><em>Exercise 2.3.1</em>:<a class="headerlink" href="#Exercise-2.3.1:" title="Permalink to this headline">¶</a></h3>
<p>In this exercise we will see how the number of features influences the classifier, use the provided <code class="docutils literal notranslate"><span class="pre">feature_curve()</span></code> script in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">2</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module for this. The script simply selects the first 1, 2 features and so forth. If you want, you can sort your features first, so they will be added in the order you specified. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">feature_order</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">feature_order</span><span class="p">]</span>
</pre></div>
</div>
<p><p><img alt="fe8c3d4d8c774f8db97640e90638425f" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.3.1:">
<h3><em>Question 2.3.1</em>:<a class="headerlink" href="#Question-2.3.1:" title="Permalink to this headline">¶</a></h3>
<p>What do you see happening to the errors as the number of features increases? Do you get the best performance with all features, or with less? (This depends on which features you have).</p>
<p>Type your answer here</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">feature_curve</span>

<span class="n">feature_curve</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><p><img alt="3d6d84c140ee47978ce415d51de6500a" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.3.2:">
<h3><em>Question 2.3.2</em>:<a class="headerlink" href="#Question-2.3.2:" title="Permalink to this headline">¶</a></h3>
<p>What do you see happening to the errors as the number of features increases? Do you get the best performance with all features, or with less? (This depends on which features you have).</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">np.random.randn()</span></code>, replace the data by completely noisy features and run the script again. What happens to the errors now? What behavior of the train error might be surprising?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">feature_curve</span>

<span class="n">feature_curve</span><span class="p">(</span><span class="n">use_random</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Type your answer here</p>
<p><p><img alt="31a611d7114345ccbfc8ecf755ba7b90" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.3.3:">
<h3><em>Question 2.3.3</em>:<a class="headerlink" href="#Question-2.3.3:" title="Permalink to this headline">¶</a></h3>
<p>Go to the website <a class="reference external" href="http://tylervigen.com/spurious-correlations">http://tylervigen.com/spurious-correlations</a> and look at the different plots. Explain in your own words, what the previous part of this question, and the phenomenon you see on the website, have in common.</p>
<p>Type your answer here</p>
<p>### 2.4. Distances in high dimensions (OPTIONAL)</p>
<p>You might already have the idea that data in high dimensional feature spaces has characteristics that do not correspond to our intuition of how things behave in 2D or 3D. This exercise looks at one of the reasons this happens, by looking at the distribution of the distances.</p>
<p><p><img alt="23f48170e40a49268860796e82e4609f" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.4.1:">
<h3><em>Exercise 2.4.1</em>:<a class="headerlink" href="#Exercise-2.4.1:" title="Permalink to this headline">¶</a></h3>
<p>Use the provided <code class="docutils literal notranslate"><span class="pre">high_dimensions_demo()</span></code> script <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">2</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module to generate 100 samples from a 2D Gaussian distribution, compute all pairwise Euclidean distances and make a histogram of the distances.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">high_dimensions_demo</span>

<span class="n">high_dimensions_demo</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2D Gaussian distribution
Mean = 1.7119170239464965, Max = 6.124070966243419, Mean nn = 48.76
1000D Gaussian distribution
Mean = 44.338243877607304, Max = 48.60148649483076, Mean nn = 46.5
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/reader_2.3_CAD_generalization_overfitting_28_1.png" src="../_images/reader_2.3_CAD_generalization_overfitting_28_1.png" />
</div>
</div>
<p><p><img alt="3acf0faf87e54ee3b4af6e0a32b86c5f" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.4.1:">
<h3><em>Question 2.4.1</em>:<a class="headerlink" href="#Question-2.4.1:" title="Permalink to this headline">¶</a></h3>
<p>What average, and what maximum distance do you observe? What is the average nearest neighbor distance?</p>
<p>Type your answer here</p>
<p><p><img alt="7d2ca355007e4e9bb76452c2ce045281" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.4.2:">
<h3><em>Question 2.4.2</em>:<a class="headerlink" href="#Question-2.4.2:" title="Permalink to this headline">¶</a></h3>
<p>The second subplot of the resulting figure above shows the result when a 1000D Gaussian distribution is used instead of a 2D one. How do the distances change? What does this tell you about finding nearest neighbors in high dimensions?</p>
<p>Type your answer here</p>
<p>### 2.5. More surprising properties of high dimensions (OPTIONAL)</p>
<p>Think about 100 uniformly distributed samples on a unit line, unit square, unit cube, etc. If it helps your thinking, think about 100 people standing on a line of 100 meters, on a field of 100x100, or spread out in a 100x100x100 building.</p>
<p><p><img alt="45bd5f8219254316bbf08f74b6e72aab" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.5.1:">
<h3><em>Question 2.5.1</em>:<a class="headerlink" href="#Question-2.5.1:" title="Permalink to this headline">¶</a></h3>
<p>Let’s take the line. How far does a person on average need to travel along the line, to find 1% of the others (i.e. 1 person)?</p>
<p>Type your answer here</p>
<p><p><img alt="876be1ee57034bc8b9a46f16938ea390" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.5.2:">
<h3><em>Question 2.5.2</em>:<a class="headerlink" href="#Question-2.5.2:" title="Permalink to this headline">¶</a></h3>
<p>What about the square - how far does a person need to travel <em>in each direction</em> to find another person? Now it is 10 meters - by travelling 10 meters in each direction, you have covered 100<span class="math notranslate nohighlight">\(m^{2}\)</span>, which is 1% of the total space available.</p>
<p>Type your answer here</p>
<p><p><img alt="3191be7bf4d34e6d8a7dbbbe3be43dd4" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.5.3:">
<h3><em>Question 2.5.3</em>:<a class="headerlink" href="#Question-2.5.3:" title="Permalink to this headline">¶</a></h3>
<p>We can calculate this for any dimensionality with the formula <span class="math notranslate nohighlight">\(r^m = V\)</span>, where <span class="math notranslate nohighlight">\(r\)</span> is the fraction that needs to be travelled (i.e. 0.01 for 1%), <span class="math notranslate nohighlight">\(m\)</span> is the dimensionality, and <span class="math notranslate nohighlight">\(V\)</span> is the volume of the data we are searching for (again 0.01 in this example).</p>
<p>Have a look at the output plots of <code class="docutils literal notranslate"><span class="pre">high_dimensions_demo()</span></code>, which shows the fraction travelled against the number of dimensions, for 1 to 10 dimensions. How far do you need to travel to find a nearest neighbor in 10 dimensions? What does this mean for the concept of “neighborhood” in high-dimensional spaces?</p>
<p>Type your answer here</p>
<p>### 2.6. <span class="math notranslate nohighlight">\(k\)</span>-Different features, different samples (OPTIONAL)</p>
<p>Recall that <span class="math notranslate nohighlight">\(k\)</span>-NN is sensitive to scaling. Because of this, some features will have a much bigger influence on the classifier than the others. To remove such differences, features are often normalized to “zero mean, unit variance”, as in the <code class="docutils literal notranslate"><span class="pre">normalize_data</span></code> function. This can be done in two ways:</p>
<ul class="simple">
<li><p>Normalizing ALL data, before splitting it up into training and test data</p></li>
<li><p>Normalizing the training data, and then applying the same normalization to the test data, by providing the test data as the second input to the <code class="docutils literal notranslate"><span class="pre">normalize_data</span></code> function.</p></li>
</ul>
<p>It is NOT correct to normalize the training and test data separately. To understand why, think of the Alice/Bob/Carol data as the training set, and Dave and Earl as the test set:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 41%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>person</p></th>
<th class="head"><p>weight (kg)</p></th>
<th class="head"><p>height (m)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Alice</p></td>
<td><p>55</p></td>
<td><p>1.6</p></td>
</tr>
<tr class="row-odd"><td><p>Bob</p></td>
<td><p>60</p></td>
<td><p>1.7</p></td>
</tr>
<tr class="row-even"><td><p>Carol</p></td>
<td><p>65</p></td>
<td><p>1.8</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 41%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>person</p></th>
<th class="head"><p>weight (kg)</p></th>
<th class="head"><p>height (m)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dave</p></td>
<td><p>65</p></td>
<td><p>1.8</p></td>
</tr>
<tr class="row-odd"><td><p>Earl</p></td>
<td><p>75</p></td>
<td><p>1.9</p></td>
</tr>
</tbody>
</table>
<p><p><img alt="7216e4c0f01f447596a33af0d4a765ea" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.6.1:">
<h3><em>Question 2.6.1</em>:<a class="headerlink" href="#Question-2.6.1:" title="Permalink to this headline">¶</a></h3>
<p>What goes wrong if you normalize these datasets separately?</p>
<p>Type your answer here</p>
<p>So far you have been using all the features for the classifier, but it is possible to only select a subset. You can experiment with this as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Normalize data</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">normalize_data</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">)</span>
<span class="c1"># Define which features to select</span>
<span class="n">ix</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="c1"># Train the classifier</span>
<span class="n">pred_labels</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">knn_classifier</span><span class="p">(</span><span class="n">train_data</span><span class="p">[:,</span> <span class="n">ix</span><span class="p">],</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_data</span><span class="p">[:,</span> <span class="n">ix</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><p><img alt="651763b65369469d925fca42cc925336" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.6.2:">
<h3><em>Question 2.6.2</em>:<a class="headerlink" href="#Question-2.6.2:" title="Permalink to this headline">¶</a></h3>
<p>Think about the scatterplots you created last week, and which features seemed to be better for the brain/non brain problem. Experiment with selecting one or more of these features. Do not forget to normalize your data first. Can you improve your classifier performance? How many possible combinations of features are there in total?</p>
<p>Type your answer here</p>
<p><p><img alt="1cfccf11fe4f47f9b523d8920da74940" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.6.1:">
<h3><em>Exercise 2.6.1</em>:<a class="headerlink" href="#Exercise-2.6.1:" title="Permalink to this headline">¶</a></h3>
<p>To avoid trying all combinations you could create a “forward feature selection” loop where you first select the best feature, based on its performance on the training set (NOT the test set - you can only use it to evaluate the final classier), then select the feature that gives the best combination with the first feature and so forth.</p>
<p><p><img alt="82f05fbaa0bc4f2e96e171ce2bf14e64" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.6.2:">
<h3><em>Exercise 2.6.2</em>:<a class="headerlink" href="#Exercise-2.6.2:" title="Permalink to this headline">¶</a></h3>
<p>Instead of selecting features, let’s experiment with extracting features with PCA. For a training and a test set, this is similar to scaling features: perform PCA on all data together, or perform PCA only on the training set, and then apply the same rotation to the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_pca</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">fraction_variance</span> <span class="o">=</span> <span class="n">seg</span><span class="o">.</span><span class="n">mypca</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">test_pca</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">test_pca</span> <span class="o">=</span> <span class="n">test_pca</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<p><p><img alt="4287bfd3f4074232aa6cee813fcfce0a" class="no-scaled-link" src="../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.6.3:">
<h3><em>Question 2.6.3</em>:<a class="headerlink" href="#Question-2.6.3:" title="Permalink to this headline">¶</a></h3>
<p>How many principal components do you need to retain at least 0.9 fraction of variance? How does the performance compare to using all features, and to using your feature subset you selected yourself?</p>
<p>Type your answer here</p>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="2.4_CAD_logistic_regression.html" class="btn btn-neutral float-right" title="Topic 2.4: Logistic regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="2.2_CAD_kNN_decision_trees.html" class="btn btn-neutral float-left" title="Topic 2.2: k-Means, k-Nearest Neighbors, decision trees" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Daniel Krahulec.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>