

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Project 2: Computer-aided diagnosis &mdash; Medical Image Analysis (8DC00) v0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Topic 2.6: (Un)supervised learning, PCA" href="2.6_CNNs_unsupervised_learning_PCA.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Medical Image Analysis (8DC00)
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="0.1_Python_programming_skills.html">Topic 0.1: Course practicalities</a></li>
</ul>
<p class="caption"><span class="caption-text">Registration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="1.1_Registration_geometrical-transformations.html">Topic 1.1: Geometrical transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.2_Registration_point-based-registration.html">Topic 1.2: Point-based registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.3_Registration_intensity-based-registration.html">Topic 1.3: Intensity-based registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.5_Registration_active-shape-models.html">Topic 1.5: Active shape models</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.6_Registration_project.html">Project 1: Medical image registration</a></li>
</ul>
<p class="caption"><span class="caption-text">Computer-aided diagnosis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="2.1_CAD_linear_regression.html">Topic 2.1: Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.2_CAD_kNN_decision_trees.html">Topic 2.2: k-Means, k-Nearest Neighbors, decision trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.3_CAD_generalization_overfitting.html">Topic 2.3: Generalization and overfitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.4_CAD_logistic_regression.html">Topic 2.4: Logistic regression</a></li>
</ul>
<p class="caption"><span class="caption-text">Convolutional neural networks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="2.5_CNNs_fundamental_building_blocks.html">Topic 2.5: Fundamental building blocks of neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.6_CNNs_unsupervised_learning_PCA.html">Topic 2.6: (Un)supervised learning, PCA</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Project 2: Computer-aided diagnosis</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Goal:">Goal:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Deliverables:">Deliverables:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Reading-assignment:">Reading assignment:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Task-1:"><em>Task 1</em>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Assessment:">Assessment:</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Guided-project-work">Guided project work</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#1.-Linear-regression-for-nuclei-area-measurement">1. Linear regression for nuclei area measurement</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Task-2:"><em>Task 2</em>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Task-3:"><em>Task 3</em>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#2.-Logistic-regression-for-nuclei-classification">2. Logistic regression for nuclei classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Task-4:"><em>Task 4</em>:</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Medical Image Analysis (8DC00)</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Project 2: Computer-aided diagnosis</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/reader/2.8_CAD_project.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
<section id="Project-2:-Computer-aided-diagnosis">
<h1>Project 2: Computer-aided diagnosis<a class="headerlink" href="#Project-2:-Computer-aided-diagnosis" title="Permalink to this headline">¶</a></h1>
<p><p><img alt="727de018975a4c55b2fa7526723c2d2b" class="no-scaled-link" src="../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="Goal:">
<h2>Goal:<a class="headerlink" href="#Goal:" title="Permalink to this headline">¶</a></h2>
<p>Implement and apply linear regression and logistic regression for measuring nuclei size in histopathology images, and evaluate and analyze the results.</p>
<p>The size of the cell nuclei of the tumor in breast cancer patients can be indicative of the outcome. Large nuclei size indicates more aggressive tumor and in turn worse prognosis for the patient. As part of their routine work, pathologists make qualitative evaluation of the the size of the nuclei by examining the tissue under a microscope. Quantitative measurement (e.g. by manual segmentation) is a much better solution, however, it is unfeasible as it takes additional time away from the busy
pathologists. A solution to this problem is to develop an automatic method for measurement of nuclei area.</p>
<p>All data required for this mini-project is provided with the code handout. In the exercises, you applied regression and classification methods on toy datasets, and in the project work you will apply the same methods to a dataset of RGB images of nuclei with size <span class="math notranslate nohighlight">\(24\times24\)</span> pixels. The images originate from the dataset that was previously described in <a class="reference external" href="#references">Veta et al. (2015)</a>.</p>
</section>
<section id="Deliverables:">
<h2>Deliverables:<a class="headerlink" href="#Deliverables:" title="Permalink to this headline">¶</a></h2>
<p>There is no hard limit for the length of the report, however, concise and short reports are <strong>strongly</strong> encouraged. Aim to present your most important findings in the main body of the report and (if needed) any additional information in an appendix. The following report structure is suggested for the main body of the report:</p>
<ol class="arabic simple">
<li><p>Introduction</p></li>
<li><p>Methods</p></li>
<li><p>Results</p></li>
<li><p>Discussion</p></li>
<li><p>Reading assignment (see below)</p></li>
</ol>
<p>The introduction and result sections can be very brief in this case (e.g. half a page each). The discussion section should contain the analysis of the results.</p>
<p>The report must be submitted as a single PDF file. The code must be submitted as a single archive file (e.g. zip) that is self-contained and can be used to reproduce the results in the report.</p>
<p>Note that there is not a single correct solution for the project. You have to demonstrate to the reader that you understand the methods that you have studied and can critically analyze the results of applying the methods. Below, you can find a set of assignments (guided project work) that will help you get started with the project work and when correctly completed will present you with a <strong>minimal solution</strong>. Solutions which go beyond these assignments are of course encouraged.</p>
<p>Code and a report describing your implementation, results and analysis.</p>
</section>
<section id="Reading-assignment:">
<h2>Reading assignment:<a class="headerlink" href="#Reading-assignment:" title="Permalink to this headline">¶</a></h2>
<p>In recent literature, various deep learning-based methods have been proposed for cell nuclei segmentation and classification. In this reading assignment, you are asked to carefully study the paper by <a class="reference external" href="#references">Graham et al. (2019)</a>.</p>
<p><p><img alt="d220fbb656384192b2868542c059207b" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Task-1:">
<h2><em>Task 1</em>:<a class="headerlink" href="#Task-1:" title="Permalink to this headline">¶</a></h2>
<p>In a separate section of your project report (~ half a page), compare your own linear and logistic regression-based methods with the deep neural network proposed by Graham et al. (2019). Start with giving a brief summary of the proposed method. What are the advantages of this method, and what are its weak points/disadvantages?</p>
</section>
<section id="Assessment:">
<h2>Assessment:<a class="headerlink" href="#Assessment:" title="Permalink to this headline">¶</a></h2>
<p>The rubric that will be used for assessment of the project work is given in <a class="reference external" href="../rubric.md">this table</a></p>
<section id="Guided-project-work">
<h3>Guided project work<a class="headerlink" href="#Guided-project-work" title="Permalink to this headline">¶</a></h3>
<p><p><img alt="b4fd68f0f7da4c679d6a0297a87ee669" class="no-scaled-link" src="../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
</section>
<section id="1.-Linear-regression-for-nuclei-area-measurement">
<h2>1. Linear regression for nuclei area measurement<a class="headerlink" href="#1.-Linear-regression-for-nuclei-area-measurement" title="Permalink to this headline">¶</a></h2>
<p>The Python function <code class="docutils literal notranslate"><span class="pre">nuclei_measurement()</span></code> implements training of a linear regression model for measuring the area of nuclei in microscopy images. The dataset for this problem consists of small RGB images of size <span class="math notranslate nohighlight">\(24 \times 24\)</span> pixels with a nucleus in the center. Such images can be obtained, for example, by cropping from larger images after performing a nuclei detection step. The targets are the areas of the nucleus in the center of the image obtained by manual measurement. The linear
regression model that we are going to train will enable us to automatically measure the size of new, previously unseen samples (without resorting to manual measurement).</p>
<p>The first section of code loads and prepares the dataset. The data is already split into a training and testing set, each containing more than <span class="math notranslate nohighlight">\(20,000\)</span> samples (a validation dataset is not needed as we are not going to perform model selection, i.e. we are going to stick to linear regression). The last few lines of the first section of code visualise the <span class="math notranslate nohighlight">\(300\)</span> smallest and <span class="math notranslate nohighlight">\(300\)</span> largest nuclei in the training dataset.</p>
<p>In this example, we are not going to perform feature extraction but use the raw pixel values as features. Since each sample is an RGB image with size <span class="math notranslate nohighlight">\(24 \times 24\)</span> pixels, we end up with <span class="math notranslate nohighlight">\(24 \times 24 \times 3=1728\)</span> features. Locate the code that reshapes each image into a feature vector and make sure you understand how it works.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">cad_project</span> <span class="kn">import</span> <span class="n">nuclei_measurement</span>

<span class="n">nuclei_measurement</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/reader_2.8_CAD_project_4_0.png" src="../_images/reader_2.8_CAD_project_4_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/reader_2.8_CAD_project_4_1.png" src="../_images/reader_2.8_CAD_project_4_1.png" />
</div>
</div>
<p><p><img alt="b0502d75ff754d689ffb5d4186b5f104" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Task-2:">
<h2><em>Task 2</em>:<a class="headerlink" href="#Task-2:" title="Permalink to this headline">¶</a></h2>
<p>Implement the missing functionality for training a linear regression model for automatic measurement of the nuclei area. Evaluate the performance on the independent test dataset. The next lines of code plot the predicted vs. the actual area. What is your analysis of the results shown in the plot?</p>
<p><p><img alt="815602aac021423386e285cb64381ca2" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Task-3:">
<h2><em>Task 3</em>:<a class="headerlink" href="#Task-3:" title="Permalink to this headline">¶</a></h2>
<p>Train a new linear regression model with a reduced number of training samples. Which model results in larger error on the testing set and why?</p>
</section>
<section id="2.-Logistic-regression-for-nuclei-classification">
<h2>2. Logistic regression for nuclei classification<a class="headerlink" href="#2.-Logistic-regression-for-nuclei-classification" title="Permalink to this headline">¶</a></h2>
<p>The Python function <code class="docutils literal notranslate"><span class="pre">nuclei_classification()</span></code> implements the training of a logistic regression model that classifies nuclei into the classes “large” (class label <span class="math notranslate nohighlight">\(y = 1\)</span>) and “small” (class label <span class="math notranslate nohighlight">\(y = 0\)</span>). Examine the code and comments and make sure that you understand what it does. One notable difference from before is that this code uses the analytical expression for the gradient of the loss function, instead of computing it numerically with <code class="docutils literal notranslate"><span class="pre">ngradient</span></code> as before. Using
<code class="docutils literal notranslate"><span class="pre">ngradient</span></code> will also work, but is much slower. The script is mostly complete. The only missing component is the values for the parameters of the training process.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">cad_project</span> <span class="kn">import</span> <span class="n">nuclei_classification</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>

<span class="n">nuclei_classification</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><p><img alt="175df9037ed2452798b0654547690ef9" class="no-scaled-link" src="../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Task-4:">
<h2><em>Task 4</em>:<a class="headerlink" href="#Task-4:" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>Select values for the learning rate, batch size and number of iterations (these are sometimes called hyper-parameters of the model), as well as initial values for the model parameters that will result in fast training of an accurate model for this classification problem. Note that if you don’t choose the hyper-parameters and initial parameters well, the resulting loss might be out of range of the plot.</p>
<p>Experiment with a few variations of the parameters and analyse and compare the resulting loss curves. Describe how the different hyper-parameters influence the training process.</p>
</li>
<li><p>Instead of running gradient descent for a fixed number of iterations, can you propose a stopping criterion for the training?</p></li>
<li><p>Report the classification accuracy for your best trained model.</p></li>
<li><p>Reduce the size of the training set by a very large factor (e.g. 0.5% of the original number of samples). Train the model with this reduced number of samples. Does the model overfit the training dataset? How did you come to this conclusion?</p></li>
</ol>
<p>## References [1] Veta M., van Diest P.J., Pluim J.P.W. 2016. Cutting Out the Middleman: Measuring Nuclear Area in Histopathology Slides Without Segmentation. Medical Image Computing and Computer-Assisted Intervention. <a class="reference external" href="https://www.doi.org/10.1007/978-3-319-46723-8_73">https://www.doi.org/10.1007/978-3-319-46723-8_73</a></p>
<p>[2] Graham, S., Vu, Q. D., Raza, S. E. A., Azam, A., Tsang, Y. W., Kwak, J. T., &amp; Rajpoot, N. 2019. Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. Medical Image Analysis, 58, 101563. <a class="reference external" href="https://doi.org/10.1016/j.media.2019.101563">https://doi.org/10.1016/j.media.2019.101563</a></p>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="2.6_CNNs_unsupervised_learning_PCA.html" class="btn btn-neutral float-left" title="Topic 2.6: (Un)supervised learning, PCA" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Daniel Krahulec.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>