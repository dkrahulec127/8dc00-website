

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Topic 2.2: k-Means, k-Nearest Neighbors, decision trees &mdash; Medical Image Analysis (8DC00) v0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Medical Image Analysis (8DC00)
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Topic 2.2: k-Means, k-Nearest Neighbors, decision trees</a><ul>
<li><a class="reference internal" href="#1.-k-Means-clustering">1. k-Means clustering</a><ul>
<li><a class="reference internal" href="#Exercise-1.1:"><em>Exercise 1.1</em>:</a></li>
<li><a class="reference internal" href="#Exercise-1.2:"><em>Exercise 1.2</em>:</a></li>
<li><a class="reference internal" href="#Exercise-1.3:"><em>Exercise 1.3</em>:</a></li>
<li><a class="reference internal" href="#Exercise-1.4:"><em>Exercise 1.4</em>:</a></li>
<li><a class="reference internal" href="#Exercise-1.5:"><em>Exercise 1.5</em>:</a></li>
<li><a class="reference internal" href="#Exercise-1.6:"><em>Exercise 1.6</em>:</a></li>
<li><a class="reference internal" href="#Exercise-1.7:"><em>Exercise 1.7</em>:</a></li>
</ul>
</li>
<li><a class="reference internal" href="#2.-Nearest-neighbor-classifier">2. Nearest neighbor classifier</a><ul>
<li><a class="reference internal" href="#Question-2.1:"><em>Question 2.1</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.1:"><em>Exercise 2.1</em>:</a></li>
<li><a class="reference internal" href="#Question-2.2:"><em>Question 2.2</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.2:"><em>Exercise 2.2</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.3:"><em>Exercise 2.3</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.4:"><em>Exercise 2.4</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.5:"><em>Exercise 2.5</em>:</a></li>
<li><a class="reference internal" href="#Question-2.3:"><em>Question 2.3</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.6:"><em>Exercise 2.6</em>:</a></li>
<li><a class="reference internal" href="#Question-2.4:"><em>Question 2.4</em>:</a></li>
<li><a class="reference internal" href="#Question-2.5:"><em>Question 2.5</em>:</a></li>
<li><a class="reference internal" href="#Question-2.6:"><em>Question 2.6</em>:</a></li>
<li><a class="reference internal" href="#Question-2.7:"><em>Question 2.7:</em></a></li>
<li><a class="reference internal" href="#Exercise-2.1.1:"><em>Exercise 2.1.1</em>:</a></li>
<li><a class="reference internal" href="#Question-2.1.1:"><em>Question 2.1.1</em>:</a></li>
<li><a class="reference internal" href="#Question-2.1.2:"><em>Question 2.1.2</em>:</a></li>
<li><a class="reference internal" href="#Exercise-2.1.2:"><em>Exercise 2.1.2</em>:</a></li>
<li><a class="reference internal" href="#Question-2.1.3:"><em>Question 2.1.3</em>:</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#3.-Decision-trees-(theory)">3. Decision trees (theory)</a></li>
</ul>
</div>
            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Medical Image Analysis (8DC00)</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Topic 2.2: k-Means, k-Nearest Neighbors, decision trees</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/reader/reader/2.2_CAD_kNN_decision_trees.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<section id="Topic-2.2:-k-Means,-k-Nearest-Neighbors,-decision-trees">
<h1>Topic 2.2: k-Means, k-Nearest Neighbors, decision trees<a class="headerlink" href="#Topic-2.2:-k-Means,-k-Nearest-Neighbors,-decision-trees" title="Permalink to this headline">¶</a></h1>
<p>This notebook combines theory with exercises to support the understanding of k-Means clustering, the Nearest Neighbors classifier, and decision trees in comptuter-aided diagnostics. Implement all functions in the <code class="docutils literal notranslate"><span class="pre">code</span></code> folder of your cloned repository, and test it in this notebook after implementation by importing your functions to this notebook. Use available markdown sections to fill in your answers to questions as you proceed through the notebook.</p>
<p><strong>Contents:</strong></p>
<ol class="arabic simple">
<li><p><a class="reference external" href="#kmeans">k-Means clustering</a></p></li>
<li><p><a class="reference external" href="#knnclassifier">Nearest neighbor classifier</a> 2.1 <a class="reference external" href="#brainseg">Brain segmentation example</a></p></li>
<li><p><a class="reference external" href="#decisiontrees">Decision trees (theory)</a></p></li>
</ol>
<p><p><img alt="08d409fa446e4f23bc448c703aacccb8" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="1.-k-Means-clustering">
<h2>1. k-Means clustering<a class="headerlink" href="#1.-k-Means-clustering" title="Permalink to this headline">¶</a></h2>
<p><span class="math notranslate nohighlight">\(k\)</span>-means clustering is an unsupervised machine learning approach that aims to partition data points into <span class="math notranslate nohighlight">\(k\)</span>-clusters based on similarity among data points. The term clustering refers to the automatic recognition of different clumps of points in space. Similarity between the data points is determined by the distance between them. Most often, the Euclidean distance is used, but other methods for similarity calculation exist (e.g. cosine similarity, average distance, etc.). The choice
of similarity measure depends on the problem at hand. In principle, <span class="math notranslate nohighlight">\(k\)</span>-means clustering minimizes the distance between similar clusters, while maximizing the distance between dissimilar ones.</p>
<p>Given a clustering task, the <span class="math notranslate nohighlight">\(k\)</span>-means algorithm generally proceeds as follows:</p>
<p><img alt="4c5b0590efeb4b3baa44269dcff66cb9" class="no-scaled-link" src="../../_images/kmeans_table.png" style="width: 500px; height: 300px;" /></p>
<p>To better understand the principles of <span class="math notranslate nohighlight">\(k\)</span>-means clustering, navigate to this <a class="reference external" href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">Interactive Tool for Visualizing K-Means Clustering</a>, where you can also learn more details about the <span class="math notranslate nohighlight">\(k\)</span>-means algorithm properties, initialization strategies, and performance.</p>
<p>As you can see in <a class="reference external" href="#exercise1">Exercise 1</a>, specifying the <code class="docutils literal notranslate"><span class="pre">w_initial</span></code> parameter will help overcome the issue of differences in final clusters due to random initialization.</p>
<p>Summarizing the positives of using <span class="math notranslate nohighlight">\(k\)</span>-means algorithms for your classification tasks, this approach is easy to interpret, relatively fast, scalable, and guarantees convergence. On the contrary, the number of clusters must be pre-defined, which may be challenging; <span class="math notranslate nohighlight">\(k\)</span>-means cannot cope with nonlinear boundaries between separated clusters of data points; it is highly sensitive to outliers and it tends to be slower with increasing number of samples.</p>
<p><p><img alt="c9093f97a3cd44ca8b29ed87a9701c73" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="Exercise-1.1:">
<h3><em>Exercise 1.1</em>:<a class="headerlink" href="#Exercise-1.1:" title="Permalink to this headline">¶</a></h3>
<p>Generate Gaussian data <code class="docutils literal notranslate"><span class="pre">test_data</span></code> with 100 samples per class. To start the k-Means algorithm, we will first initialize some cluster centers for k-Means to start with. Define the number of clusters, e.g. <code class="docutils literal notranslate"><span class="pre">num_clusters</span> <span class="pre">=</span> <span class="pre">2</span></code>. Now select <code class="docutils literal notranslate"><span class="pre">num_clusters</span></code> rows from <code class="docutils literal notranslate"><span class="pre">X</span></code> and store them in <code class="docutils literal notranslate"><span class="pre">w_initial</span></code>. You can verify by plotting the datasets on top of each other in different colors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">segmentation_util</span> <span class="k">as</span> <span class="nn">util</span>
<span class="kn">from</span> <span class="nn">segmentation</span> <span class="kn">import</span> <span class="n">generate_gaussian_data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_clusters</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">generate_gaussian_data</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="c1"># Generates 100 samples per Gaussian class</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">test_data</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">scatter_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">num_clusters</span><span class="p">)</span>
<span class="n">w_initial</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">,:]</span>
<span class="n">im2</span>  <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">w_initial</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">w_initial</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;w_initial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cluster indices: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cluster centers: </span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w_initial</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Cluster indices: [ 74 184]
Cluster centers:
[[-0.24725892  1.11307541]
 [ 2.28110572  1.23451529]]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.2_CAD_kNN_decision_trees_4_1.png" src="../../_images/reader_reader_2.2_CAD_kNN_decision_trees_4_1.png" />
</div>
</div>
<p><p><img alt="4fdee1b8565243c5872f82de8d79a474" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-1.2:">
<h3><em>Exercise 1.2</em>:<a class="headerlink" href="#Exercise-1.2:" title="Permalink to this headline">¶</a></h3>
<p>Assume these points are the cluster centers for your data. Calculate the minimum distances using the approach from the previous exercise and look at <code class="docutils literal notranslate"><span class="pre">min_index</span></code> to find out which cluster each sample belongs to.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">D</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w_initial</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="c1"># For each row/sample in D, which column has the minimum value...</span>
<span class="c1"># (i.e. to which point in w_initial is this sample the closest)</span>
<span class="n">min_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;min_index:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">min_index</span><span class="p">))</span>
<span class="c1"># Show these minimum distances in a vector</span>
<span class="n">min_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="o">*</span><span class="n">min_index</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">D</span><span class="p">)):</span>
    <span class="n">min_dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">min_index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">min_dist</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">min_dist</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="c1"># Calculate how many samples in X are closest to each of the samples in w_initial</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_initial</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_initial</span><span class="p">)):</span>
    <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">min_index</span><span class="o">==</span><span class="n">j</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
min_index:
[0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1
 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1
 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]
574.9930674426226
[101.  99.]
</pre></div></div>
</div>
<p><p><img alt="8a7f30fd2a3c4040bb28fb02b659c488" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-1.3:">
<h3><em>Exercise 1.3</em>:<a class="headerlink" href="#Exercise-1.3:" title="Permalink to this headline">¶</a></h3>
<p>Imagine that your cluster centers, while being randomly selected, were selected in a different order. You could achieve this with <code class="docutils literal notranslate"><span class="pre">w_initial</span> <span class="pre">=</span> <span class="pre">np.array([w_initial[1,:],</span> <span class="pre">w_initial[0,:]])</span></code>. Calculate the cluster assignments again and compare them to the previous result. If you want to use these results for segmentation of an image, what kind of problems do you expect? How could you solve this problem?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">w_switched</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w_initial</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">w_initial</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]])</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w_switched</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="c1"># For each row/sample in D, which column has the minimum value...</span>
<span class="c1"># (i.e. to which point in w_switched is this sample the closest)</span>
<span class="n">min_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;min_index:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">min_index</span><span class="p">))</span>
<span class="c1"># Show these minimum distances in a vector</span>
<span class="n">min_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="o">*</span><span class="n">min_index</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">D</span><span class="p">)):</span>
    <span class="n">min_dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">min_index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">min_dist</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">min_dist</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="c1"># Calculate how many samples in X are closest to each of the samples in w_switched</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_initial</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_initial</span><span class="p">)):</span>
    <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">min_index</span><span class="o">==</span><span class="n">j</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
min_index:
[0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1
 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0
 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1
 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1]
512.4225230461032
[116.  84.]
</pre></div></div>
</div>
<p><p><img alt="67ebdf825ee94526ad7d335220aae51f" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-1.4:">
<h3><em>Exercise 1.4</em>:<a class="headerlink" href="#Exercise-1.4:" title="Permalink to this headline">¶</a></h3>
<p>To start optimization, we need to find out how good the current cluster assignment is. Use what you learned in the registration exercises, and in the course slides to complete the missing code in <code class="docutils literal notranslate"><span class="pre">cost_kmeans()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module. You can check that this is working by creating good or bad cluster centers by hand, and then calculating the cost.</p>
<p><p><img alt="68e91e37d45c44e685386940d2baeed1" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-1.5:">
<h3><em>Exercise 1.5</em>:<a class="headerlink" href="#Exercise-1.5:" title="Permalink to this headline">¶</a></h3>
<p>Now that we have a cost function, we can optimize it with the same method as in the registration exercises. Use the code in <code class="docutils literal notranslate"><span class="pre">kmeans_demo()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module to see the final algorithm in action. What do you notice about the updates and the cost curve?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">kmeans_demo</span><span class="p">,</span> <span class="n">funX</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>

<span class="n">kmeans_cost</span> <span class="o">=</span> <span class="n">kmeans_demo</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.2_CAD_kNN_decision_trees_11_0.png" src="../../_images/reader_reader_2.2_CAD_kNN_decision_trees_11_0.png" />
</div>
</div>
<p><p><img alt="965e25d89ce546f08a3dac209b2bf9f1" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-1.6:">
<h3><em>Exercise 1.6</em>:<a class="headerlink" href="#Exercise-1.6:" title="Permalink to this headline">¶</a></h3>
<p>Now fill in the missing parts of code (parts of answers to previous questions) in <code class="docutils literal notranslate"><span class="pre">kmeans_clustering()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module.</p>
<p><p><img alt="c068d729bcd34b2684ad0b8c6886e4b6" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-1.7:">
<h3><em>Exercise 1.7</em>:<a class="headerlink" href="#Exercise-1.7:" title="Permalink to this headline">¶</a></h3>
<p>Load a slice for the brain/non-brain task. Normalize the data and apply the clustering classifier to it. Visualize the results. Implement all of this in the <code class="docutils literal notranslate"><span class="pre">kmeans_clustering_test()</span></code> script in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module. Depending on the features you use, you could see something like this:</p>
<p><img alt="8a5acbd659434253af91938b9f1b04dd" class="no-scaled-link" src="reader/notebooks/assets/kmeans_example.png" style="width: 800px;" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">kmeans_clustering_test</span>

<span class="c1">#kmeans_clustering_test()</span>
</pre></div>
</div>
</div>
<p><p><img alt="7b8425b27c5142479ae28ad75659b980" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
</section>
<section id="2.-Nearest-neighbor-classifier">
<h2>2. Nearest neighbor classifier<a class="headerlink" href="#2.-Nearest-neighbor-classifier" title="Permalink to this headline">¶</a></h2>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbor is a type of supervised learning algorithm used both in regression and classification tasks (e.g. to classify a CT or MRI scan as benign or malignant based on given features). Given <span class="math notranslate nohighlight">\(N\)</span> training vectors, the <span class="math notranslate nohighlight">\(k\)</span>-NN algorithm tries to predict the class for the test data (e.g. a feature vector <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>) by calculating the distance between <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> and other traning points. The variable <span class="math notranslate nohighlight">\(k\)</span> represents the selected number of points
which is closest to <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>.</p>
<p><img alt="4a6060d7fcc84e3e8b5eefb8d5d8ddd8" class="no-scaled-link" src="../../_images/k_nearest_neighbor.png" style="width: 500px; height: 300px;" /></p>
<p style="font-size:8px;"><p>Figure from “Antony Christopher on K-Nearest Neighbor”.</p>
</p><p>In classification tasks, <span class="math notranslate nohighlight">\(k\)</span>-NN algorithm is typically used to identify the category or class of a particular data point (or dataset) which is newly added to the space of two known categories, e.g. <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>. How does the algorithm determine the class or category? For a new example with features <span class="math notranslate nohighlight">\(x_{new} = [x_{1},x_{2}]\)</span>, predict the class <span class="math notranslate nohighlight">\(\hat{y_{new}}\)</span> as follows:</p>
<ol class="arabic simple">
<li><p>Specify the amount of neighbors <span class="math notranslate nohighlight">\(k\)</span> (e.g. <span class="math notranslate nohighlight">\(5\)</span>)</p></li>
<li><p>Compute the distance from the new point to the <span class="math notranslate nohighlight">\(k\)</span> training samples. The most frequently used distance metric is the Euclidean distance calculated as <span class="math notranslate nohighlight">\(d(x_{new},x_{i})=\sqrt{(x_{new,1}-x_{i,1})^{2} + (x_{new,2}-x_{i,2})^{2}}\)</span> (Note: another often used metric is the <cite>:math:`L_{mathrm{1}}</cite>-distance &lt;<a class="reference external" href="https://iq.opengenus.org/manhattan-distance/">https://iq.opengenus.org/manhattan-distance/</a>&gt;`__)</p></li>
<li><p>Count the number of data points in each category among the <span class="math notranslate nohighlight">\(k\)</span> neighbors according to the Euclidean distance, sort them, and pick the nearest ones</p></li>
<li><p>Determine the class of the <span class="math notranslate nohighlight">\(k\)</span> nearest training samples</p></li>
<li><p>Assign to <span class="math notranslate nohighlight">\(x_{new}\)</span> the majority class of its nearest training samples (neighbors)</p></li>
<li><p>Algorithm has finished</p></li>
</ol>
<p>Now, the question remains how to select the values of <span class="math notranslate nohighlight">\(k\)</span>. In general, the higher the value of <span class="math notranslate nohighlight">\(k\)</span>, the lesser the chance of erroneous classification. However, one has to keep in mind that every iteration of the distance calculation is computationally expensive. One cannot select the most applicable <span class="math notranslate nohighlight">\(k\)</span>-value via any pre-defined statistical methods. If we were to choose the optimal value of <span class="math notranslate nohighlight">\(k\)</span> based on the performance on the training set, we would always select
<span class="math notranslate nohighlight">\(k = 1\)</span> since the training error would be 0. Hence, we need to choose <span class="math notranslate nohighlight">\(k\)</span> based on the performance on an independent test set. The test set should be independent in the sense that the examples that it contains should by no means be related to the ones in the training set.</p>
<p><p><img alt="68aa532dd9764213a12f07ede65e6302" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="Question-2.1:">
<h3><em>Question 2.1</em>:<a class="headerlink" href="#Question-2.1:" title="Permalink to this headline">¶</a></h3>
<p>First compute the (square) distance matrix between all samples in your Gaussian dataset. Find the minimum distances min dist and the indices of the corresponding columns min index as before. Can you already predict what values will be returned and why?</p>
<p>Type your answer here</p>
<p><p><img alt="fa340fb27f93494dbf38fc63aa5f93a0" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.1:">
<h3><em>Exercise 2.1</em>:<a class="headerlink" href="#Exercise-2.1:" title="Permalink to this headline">¶</a></h3>
<p>To make things a bit more exciting, generate two separate, small Gaussian datasets with different number of samples, but other than that the same parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span><span class="p">,</span> <span class="n">train</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_gaussian_data</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
<span class="n">test_data</span><span class="p">,</span> <span class="n">test</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_gaussian_data</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
<p>Classify the points in <code class="docutils literal notranslate"><span class="pre">test_data</span></code>, based on their distances <code class="docutils literal notranslate"><span class="pre">d</span></code> to the points in <code class="docutils literal notranslate"><span class="pre">train_data</span></code>. Write your implementation in <code class="docutils literal notranslate"><span class="pre">distance_classification_test()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">distance_classification_test</span>

<span class="n">distance_classification_test</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train data:
[[ 0.83930812  0.81613018]
 [-1.32459423  0.13026253]
 [ 2.64342427  1.97285797]
 [ 1.9433268   0.12701892]]
Test data:
[[-2.6113333   1.34939897]
 [ 1.48402625 -1.15203134]]
Distance matrix:
[[3.49160448 1.77256624 5.29161396 4.71583945]
 [2.07106766 3.0874952  3.33303721 1.35901677]]
</pre></div></div>
</div>
<p><p><img alt="dd723104dd1744d89ece3704e90ac43d" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.2:">
<h3><em>Question 2.2</em>:<a class="headerlink" href="#Question-2.2:" title="Permalink to this headline">¶</a></h3>
<p>What will the dimensions of <code class="docutils literal notranslate"><span class="pre">d</span></code> be? In which position <code class="docutils literal notranslate"><span class="pre">[row,</span> <span class="pre">column]</span></code> in the matrix is the distance of the last sample from <code class="docutils literal notranslate"><span class="pre">test_data</span></code>, to the first sample of <code class="docutils literal notranslate"><span class="pre">train_data</span></code>?</p>
<p>Type your answer here</p>
<p><p><img alt="1a0745e852ec4333b79e3b77b95a27e5" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.2:">
<h3><em>Exercise 2.2</em>:<a class="headerlink" href="#Exercise-2.2:" title="Permalink to this headline">¶</a></h3>
<p>Implement the nearest neighbor functionality in the function <code class="docutils literal notranslate"><span class="pre">nn_classifier()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module, which takes as input a training dataset with labels, and an unlabeled test dataset, and outputs the predicted labels for the test dataset.</p>
<p><p><img alt="58710d10f8b2454aaa5077a53f1dde1c" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.3:">
<h3><em>Exercise 2.3</em>:<a class="headerlink" href="#Exercise-2.3:" title="Permalink to this headline">¶</a></h3>
<p>Now we will evaluate the classifier you implemented using the <code class="docutils literal notranslate"><span class="pre">nn_classifier_test_samples()</span></code> script in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module. You can inspect the predicted labels by displaying them side by side with the true labels. To count how many are incorrect, use <code class="docutils literal notranslate"><span class="pre">np.sum(true_labels</span> <span class="pre">!=</span> <span class="pre">predicted_labels)</span></code>. How can you now calculate the error as a percentage? Implement the missing line of code in <code class="docutils literal notranslate"><span class="pre">classification_error()</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">nn_classifier_test_samples</span>

<span class="n">nn_classifier_test_samples</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
True labels:
[[0.]
 [1.]]
Predicted labels:
[[0.]
 [1.]]
Error:
0.0
</pre></div></div>
</div>
<p><p><img alt="7d5f8dd2af074a41a0f34c3f2cf19f43" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.4:">
<h3><em>Exercise 2.4</em>:<a class="headerlink" href="#Exercise-2.4:" title="Permalink to this headline">¶</a></h3>
<p>Inspect the function <code class="docutils literal notranslate"><span class="pre">generate_train_test()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module, which generates a pair of training and test datasets. Modify the code (you only need to modify the parameters mu and sigma), such that the function can produce an “easy” dataset pair with low class overlap, where you expect the classification error to be low, and a “hard” dataset pair with high class overlap, where you expect the classification error to be high. Verify this using
<code class="docutils literal notranslate"><span class="pre">nn_classifier()</span></code> and <code class="docutils literal notranslate"><span class="pre">classification_error()</span></code>. Write your implementation in <code class="docutils literal notranslate"><span class="pre">easy_hard_data_classifier_test()</span></code> in <code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module.</p>
<p>For the answer, also list the parameters you used for the easy and hard settings.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">easy_hard_data_classifier_test</span>
<span class="n">easy_hard_data_classifier_test</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Easy dataset:
Error:
0.85
Hard dataset:
Error:
0.4
</pre></div></div>
</div>
<p><p><img alt="1bd9cbaced8c4d51b34d9eb6cc3fb717" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.5:">
<h3><em>Exercise 2.5</em>:<a class="headerlink" href="#Exercise-2.5:" title="Permalink to this headline">¶</a></h3>
<p>Add the functionality below to <code class="docutils literal notranslate"><span class="pre">knn_classifier()</span></code>, which is similar to <code class="docutils literal notranslate"><span class="pre">nn_classifier()</span></code> but includes an additional input <code class="docutils literal notranslate"><span class="pre">k</span></code>. Instead of finding only the minimum distance using <code class="docutils literal notranslate"><span class="pre">scipy.spatial.distance.cdist()</span></code> and <code class="docutils literal notranslate"><span class="pre">np.argmin()</span></code>, you now need to find <code class="docutils literal notranslate"><span class="pre">k</span></code> minimum distances, then make <code class="docutils literal notranslate"><span class="pre">k</span></code> predictions for the labels, and then combine the <code class="docutils literal notranslate"><span class="pre">k</span></code> predictions into a final label. The code for this is already given below (so you just need to copy+paste it):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sort_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sort_ix_k</span> <span class="o">=</span> <span class="n">sort_ix</span><span class="p">[:,:</span><span class="n">k</span><span class="p">]</span> <span class="c1"># Get the k smallest distances</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">sort_ix_k</span><span class="p">]</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">predicted_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>You can find <code class="docutils literal notranslate"><span class="pre">knn_classifier()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module.</p>
<p><p><img alt="1defbbf4e8ab4c269b7982eb1b00160c" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.3:">
<h3><em>Question 2.3</em>:<a class="headerlink" href="#Question-2.3:" title="Permalink to this headline">¶</a></h3>
<p>To make sure you understand the code, assume that you have a training set of 10 samples, a test set of 5 samples, and you are using a 3-nearest neighbor classifier. What are the dimensions of the variables <code class="docutils literal notranslate"><span class="pre">sort_ix</span></code>, <code class="docutils literal notranslate"><span class="pre">sort_ix_k</span></code>, <code class="docutils literal notranslate"><span class="pre">predicted_labels_k</span></code> and <code class="docutils literal notranslate"><span class="pre">predicted_labels</span></code>?</p>
<p>Type your answer here</p>
<p><p><img alt="99ff1eec246a49128b8fbcb0ad94698b" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.6:">
<h3><em>Exercise 2.6</em>:<a class="headerlink" href="#Exercise-2.6:" title="Permalink to this headline">¶</a></h3>
<p>Load your brain/not brain datasets for two different subjects (one subject for training, another one for testing). Subsample the training set to 100 samples. Look at the code in <code class="docutils literal notranslate"><span class="pre">knn_curve()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module, modify the values of <code class="docutils literal notranslate"><span class="pre">k</span></code>, and test the code several times. Note that to save time, the <strong>scikit-learn</strong> implementation of k-NN could be used.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">knn_curve</span>

<span class="n">knn_curve</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><p><img alt="0f3bff9e83a04e34808c3c771d38ab59" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.4:">
<h3><em>Question 2.4</em>:<a class="headerlink" href="#Question-2.4:" title="Permalink to this headline">¶</a></h3>
<p>Describe in your own words what the code does.</p>
<p>Type your answer here</p>
<p><p><img alt="b2a02db237ed4a4f846b58a566041075" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.5:">
<h3><em>Question 2.5</em>:<a class="headerlink" href="#Question-2.5:" title="Permalink to this headline">¶</a></h3>
<p>What would you say is a good value of k? You might want to repeat the above a few times to get a better estimate of the performance. How could you estimate the error at k = 100, without evaluating the classifier? Does a lower error always equal to higher Dice (and vice versa)?</p>
<p>Type your answer here</p>
<p><p><img alt="e9655efb918047bca71d2b757a4be4cd" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.6:">
<h3><em>Question 2.6</em>:<a class="headerlink" href="#Question-2.6:" title="Permalink to this headline">¶</a></h3>
<p>Here, we combine <span class="math notranslate nohighlight">\(k\)</span>-NN classifiers trained on different subjects. This functionality is also already implemented for you in the function <code class="docutils literal notranslate"><span class="pre">segmentation_combined_knn()</span></code> in <code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">SECTION</span> <span class="pre">3</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module. What would you expect from the results of this combined method, compared to training <span class="math notranslate nohighlight">\(k\)</span>-NN on only one subject? Test your hypothesis on one of the subjects. (You can use <code class="docutils literal notranslate"><span class="pre">segmentation_knn()</span></code> to do this, note that this function has an extra input, since you need
to specify which training subject you want to use!)</p>
<p>Type your answer here</p>
<p><p><img alt="1f7478fcd88b49ebbeeb1a65f077cbc9" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.7:">
<h3><em>Question 2.7:</em><a class="headerlink" href="#Question-2.7:" title="Permalink to this headline">¶</a></h3>
<p>What could you do if you wanted to combine more than four classifiers (giving different decisions) here, but you could only use a 5-NN classifier?</p>
<p>Type your answer here</p>
<p>## 2.1 Brain segmentation example</p>
<p>Create two datasets (same slice, different subjects) for the brain/not brain classification task. Downsample the training set to 1000 pixels, this will save time during calculations. The test set should not be downsampled.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">segmentation_util</span> <span class="k">as</span> <span class="nn">util</span>
<span class="kn">from</span> <span class="nn">segmentation</span> <span class="kn">import</span> <span class="n">generate_gaussian_data</span>
<span class="c1"># Subject 1, slice 1 is the train data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">feature_labels_train</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;brain&#39;</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,:]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">ix</span><span class="p">,:]</span>
<span class="c1"># Subject 3, slice 1 is the test data</span>
<span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">feature_labels_test</span>  <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;brain&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><p><img alt="a4e80f0449f9478b86f07c1a6aff3f21" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.1.1:">
<h3><em>Exercise 2.1.1</em>:<a class="headerlink" href="#Exercise-2.1.1:" title="Permalink to this headline">¶</a></h3>
<p>Train the nearest neighbor classifier on train data, apply it to test data to get <code class="docutils literal notranslate"><span class="pre">predicted_labels</span></code>, and calculate the classification error. Implement your solution in the <code class="docutils literal notranslate"><span class="pre">nn_classifier_test_brains()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">nn_classifier_test_brains</span>

<span class="n">nn_classifier_test_brains</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><p><img alt="de1c83c4888f4bfc8f4aea303780b22b" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.1.1:">
<h3><em>Question 2.1.1</em>:<a class="headerlink" href="#Question-2.1.1:" title="Permalink to this headline">¶</a></h3>
<p>What % of pixels are classified incorrectly? Run the code a few times, because of the data sampling you will get slightly different errors. Your answer to this question will probably be different from other groups, depending on the type of features you have previously implemented in the <code class="docutils literal notranslate"><span class="pre">extract_features</span></code> function.</p>
<p>Type your answer here</p>
<p><p><img alt="e06c542efc74489c89ad65c1453f1dbe" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.1.2:">
<h3><em>Question 2.1.2</em>:<a class="headerlink" href="#Question-2.1.2:" title="Permalink to this headline">¶</a></h3>
<p>Now we know, from the classification error, how many pixels are predicted to be in the wrong class. The generated images also show what the classification results look like. This visualization was created by transforming the vector of predicted labels back into an image using <code class="docutils literal notranslate"><span class="pre">np.reshape()</span></code>. We have also loaded the ground truth mask in order to compare the result.</p>
<p>How would you describe the predicted mask? What kind of pixels are often classified incorrectly and why?</p>
<p>Type your answer here</p>
<p><p><img alt="d82fade986674bf0b5336bc275280577" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-2.1.2:">
<h3><em>Exercise 2.1.2</em>:<a class="headerlink" href="#Exercise-2.1.2:" title="Permalink to this headline">¶</a></h3>
<p>Implement the missing functionality in <code class="docutils literal notranslate"><span class="pre">dice_overlap()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">1</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module and evaluate it on your segmentation. If you have a low classification error, you should have quite a high Dice score.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">nn_classifier_test_brains</span>

<span class="n">nn_classifier_test_brains</span><span class="p">(</span><span class="n">testDice</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Error:
0.0359375
Dice coefficient:
0.944693812119269
</pre></div></div>
</div>
<p><p><img alt="76db9567c0e341a1834f728d738d656d" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-2.1.3:">
<h3><em>Question 2.1.3</em>:<a class="headerlink" href="#Question-2.1.3:" title="Permalink to this headline">¶</a></h3>
<p>Can you think of an image where the error would be low, but the Dice score would be low, instead of high? If this is difficult to answer, try experimenting with different 10x1 vectors predicted labels and test labels that you fill in by hand, and vary the total number of zeros and ones in each vector.</p>
<p>Type your answer here</p>
<p><p><img alt="740df3277f464bcf96d5b90643301dc3" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
</section>
</section>
<section id="3.-Decision-trees-(theory)">
<h1>3. Decision trees (theory)<a class="headerlink" href="#3.-Decision-trees-(theory)" title="Permalink to this headline">¶</a></h1>
<p>Now that you have learnt about <span class="math notranslate nohighlight">\(k\)</span>-means and <span class="math notranslate nohighlight">\(k\)</span>-NN algorithms, it is time to have a look at a more powerful technique for classification and regression, the so-called <span class="math notranslate nohighlight">\(k\)</span>-dimensional decision tree classifiers. Decision trees are non-parametric supervised machine learning methods that aim to create a model that learns from data by repeatedly subdividing the feature space into smaller disjoint subsets to predict the value of the target variable. The input to the root of the tree
is the sample to be classified. During the splitting process (based on comparisons), the decision tree incrementally grows, resulting in a tree with decision and leaf (terminal) nodes (subtrees). At the end of the process, each subset contains only samples with the same label. Decision trees work both with categorical, and continuous variables.</p>
<p>One can also imagine a decision tree as a hierarchical flowchart diagram with initial node and terminal nodes representing the initial condition and classification decisions, respectively. Every decision node represents a point in feature space. In principle, the aim of a decision tree is to minimize the tree size to obtain the simplest possible explanation of the data. Normally, an error rate is accepted to avoid overfitting to the training data (this step is called <em>pruning</em>).</p>
<p>For illustration, see the <cite>documentation website of the ``scikit-learn`</cite> Python package &lt;<a class="reference external" href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html</a>&gt;`__, which contains decision tree algorithms.</p>
<p>It is common to use a splitting criterion to generate a tree from samples, e.g. the value distribution entropy in each subset. Ideally, a subset would only contain one label, i.e. its entropy would be minimal. Some examples of algorithms utilizing entropy are CART (Classification and Regression Trees), ID3, C4.5, and C5.</p>
<p>A great further explanation of <span class="math notranslate nohighlight">\(k\)</span>-dimensional trees together with graphical description of how the tree grows can be found <a class="reference external" href="https://iq.opengenus.org/k-dimensional-tree/">on this website</a>.</p>
</section>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Daniel Krahulec.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>