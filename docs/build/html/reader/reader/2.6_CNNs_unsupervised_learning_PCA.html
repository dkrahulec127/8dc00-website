

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Topic 2.6: (Un)supervised learning, PCA &mdash; Medical Image Analysis (8DC00) v0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Medical Image Analysis (8DC00)
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Topic 2.6: (Un)supervised learning, PCA</a></li>
<li><a class="reference internal" href="#1.-Supervised-vs. unsupervised-learning">1. Supervised vs. unsupervised learning</a></li>
<li><a class="reference internal" href="#2.-Principal-component-analysis-(theory)">2. Principal component analysis (theory)</a><ul>
<li><a class="reference internal" href="#Curse-of-dimensionality"><em>Curse of dimensionality</em></a></li>
<li><a class="reference internal" href="#Mathematical-background"><em>Mathematical background</em></a></li>
<li><a class="reference internal" href="#2.3-Dimensionality-reduction-using-PCA">2.3 Dimensionality reduction using PCA</a></li>
<li><a class="reference internal" href="#A-graphical-example:-cell-nuclei"><em>A graphical example: cell nuclei</em></a></li>
</ul>
</li>
<li><a class="reference internal" href="#3.-Principal-component-analysis-(exercises)">3. Principal component analysis (exercises)</a><ul>
<li><a class="reference internal" href="#Exercise-3.1:"><em>Exercise 3.1</em>:</a></li>
<li><a class="reference internal" href="#Question-3.1:"><em>Question 3.1</em>:</a></li>
<li><a class="reference internal" href="#Exercise-3.2:"><em>Exercise 3.2</em>:</a></li>
<li><a class="reference internal" href="#Exercise-3.3:"><em>Exercise 3.3</em>:</a></li>
<li><a class="reference internal" href="#Question-3.2:"><em>Question 3.2</em>:</a></li>
<li><a class="reference internal" href="#Exercise-3.4:"><em>Exercise 3.4</em>:</a></li>
<li><a class="reference internal" href="#Question-3.3:"><em>Question 3.3</em>:</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Medical Image Analysis (8DC00)</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Topic 2.6: (Un)supervised learning, PCA</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/reader/reader/2.6_CNNs_unsupervised_learning_PCA.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Topic-2.6:-(Un)supervised-learning,-PCA">
<h1>Topic 2.6: (Un)supervised learning, PCA<a class="headerlink" href="#Topic-2.6:-(Un)supervised-learning,-PCA" title="Permalink to this headline">¶</a></h1>
<p>This notebook combines theory with code exercises to support the understanding of (un)supervised machine learning and principal component analysis in computer-aided diagnosis. <strong>Please note:</strong> Cells in this notebook must be executed <em>in order</em> as the code often relies on cells run above it!</p>
<p><strong>Contents:</strong></p>
<ol class="arabic simple">
<li><p>Supervised vs. unsupervised learning</p></li>
<li><p>Principal component analysis (theory) 2.1 Motivation 2.2 Basics of PCA 2.3 Dimensionality reduction using PCA 2.4 Intuitive interpretations and principal components</p></li>
<li><p>Principal component analysis (exercises)</p></li>
</ol>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<p><p><img alt="5e25046efe70416a88937099f7238cfb" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="1.-Supervised-vs. unsupervised-learning">
<h1>1. Supervised vs. unsupervised learning<a class="headerlink" href="#1.-Supervised-vs. unsupervised-learning" title="Permalink to this headline">¶</a></h1>
<p>While supervised machine learning (e.g. classification or regression) serves to develop a predictive model based on input and output data (i.e. ground truth, labels), unsupervised machine learning (e.g. clustering or dimensionality reduction) utilizes machine learning algorithms to cluster unlabelled data by finding hidden patterns without the need for manual human intervention.</p>
<p><img alt="d553775ce217414793168c5217cfd8cc" class="no-scaled-link" src="../../_images/machine_learning_diagram.png" style="width: 800px; height: 300px;" /></p>
<p style="font-size:8px;"><p>Figure from Cognitive platform</p>
</p><p>Supervised learning requires a lot of known transformations for training and it is necessary to use ground truth (labels) to calculate the loss. Supervised machine learning methods can either be <em>fully supervised</em> (e.g. 2D FlowNet or 3D U-net, mostly patch-based; see <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/30371358/">generation of ground truth transformations</a> and <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/28705497/">large deformation diffeomorphic metric mapping - LDDMM</a>) or <em>weakly supervised</em>, utilizing
overlap between segmentations or a similarity metric combined with ground truth (e.g. registration between ultrasound and MRI images using <a class="reference external" href="https://arxiv.org/abs/1804.11024">convolutional neural networks - CNNs</a> or <a class="reference external" href="https://arxiv.org/abs/1807.03361">generative adversarial networks - GANs</a>).</p>
<p>Despite its relative computational complexity due to a high volume of traning data, unsupervised learning is widely used in computer vision tasks (for visual perception and object recognition) as well as in medical imaging to guide radiologists and pathologists towards accurate diagnoses. There is no need for ground truth (labels); instead, unsupervised methods often use a spatial transformer layer. A spatial transformer is a learnable module that explicitly allows for spatial manipulation of
data within a given network. They are differentiable, modularizable for existing neural networks and serve for active transformation of feature maps.</p>
<p><strong>Examples of unsupervised machine learning methods:</strong> 1. <a class="reference external" href="https://arxiv.org/abs/1804.07172">Vairational autoencoders</a> 2. <a class="reference external" href="https://arxiv.org/abs/1807.07349">Generative adversarial networks (GANs)</a> 3. Multi-scale methods: 3.1 <a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-319-66182-7_27">RegNet</a> 3.2 <a class="reference external" href="https://arxiv.org/abs/1809.06130">ConvNet</a> 3.3 <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/31751269/">pgCNN</a> 3.4 <a class="reference external" href="https://arxiv.org/abs/1812.06499">HoVer-Net</a> 4. <a class="reference external" href="https://arxiv.org/abs/1809.05231">VoxelMorph
(U-net)</a> 4.1 <a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-030-32226-7_19">Cycle-consistent VoxelMorph</a></p>
<p><p><img alt="5e2a8310dbf54c82877e860e4843d203" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="2.-Principal-component-analysis-(theory)">
<h1>2. Principal component analysis (theory)<a class="headerlink" href="#2.-Principal-component-analysis-(theory)" title="Permalink to this headline">¶</a></h1>
<p>This section is meant to teach you about <strong>principal component analysis</strong> (PCA), a technique that is commonly used in medical image analysis to reduce the dimensionality of data (for example, features for a classification or segmentation task).</p>
<p>We will begin with some motivation <em>why</em> PCA (and dimensionality reduction in general) is useful. Next, we will show PCA applied to a simple Gaussian dataset, and then to the nuclei dataset.</p>
<p><p><img alt="00b0bea000474e788684b3f24350a8f0" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p><p>### 2.1 Motivation The ultimate goal of any classification/segmentation technique is to use training data to make accurate predictions about future, unlabeled data. In other words, the goal is to <em>generalize</em> well to new data. This can be achieved by many different choices for classifiers and many different combinations of features. For example, when tasked with predicting the systolic blood pressure of a patient given some clinical data, we could use many different clinical values: the
low-density lipoprotein (LDL) blood levels of the patient, their high-level lipoprotein (HDL), but also their age, weight and height.</p>
<p>Choosing which combination of features to use for a classifier is a non-trivial task, for a variety of reasons: * For most complex tasks, it is inherently unclear what number of features is ‘enough’. * It may not always be intuitive what features are actually discriminatory between your classes. * Increasing the number of features may actually decrease the performance of the classifier.</p>
<p><img alt="title" src="../../_images/1Dproblem.png" /></p>
<div style="text-align: center"><p>Figure 2.1.1: As dimensionality (the number of features) increases, a classifier’s performance can increase until some optimum. Further increasing the dimensionality for the same amount of training samples can result in a decrease in classifier performance.</p>
</div><section id="Curse-of-dimensionality">
<h2><em>Curse of dimensionality</em><a class="headerlink" href="#Curse-of-dimensionality" title="Permalink to this headline">¶</a></h2>
<p>It is worth elaborating on the last point: increasing the number of features may inadvertedly lower classifier performance. This is because of something called the <em>curse of dimensionality</em>. We can illustrate the curse of dimensionality with a simple example, where we try to classify pictures of dogs from cats using a linear classifier.</p>
<p>In the image below, we show how the feature space of the classifier grows by adding one new feature (dimension) at a time. You can image that we might begin by classifying dogs from cats by their color, but we find that this doesn’t offer a linear seperability of our dataset (there is no one line we can draw that perfectly seperates the two animals). We see the same for two features (e.g. color and size). However, when we use three features (e.g. color, size and weight), we <em>can</em> linearly
seperate the two classes, thus giving us perfect classification accuracy!</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30%" />
<col style="width: 35%" />
<col style="width: 35%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><img alt="title" src="../../_images/1Dproblem.png" /></p></td>
<td><p><img alt="image1" src="../../_images/2Dproblem.png" /></p></td>
<td><p><img alt="image2" src="../../_images/3Dproblem_separated.png" /></p></td>
</tr>
</tbody>
</table>
<div style="text-align: center"><p>Figure 2.2.2: The more features we use, the higher the likelihood we can seperate the classes with a hyperplane.</p>
</div><p>While it looks like adding more features has improved classification performance, <strong>this is actually not the case</strong>. Consider the ‘density’ of all of the training samples (represented by cat/dog icons) within the feature space: as the number of dimensions goes up, the feature space becomes increasingly sparse. As you might intuitively feel, it is much easier to find a hyperplane that splits two classes perfectly in a sparse feature space than in a very densely filled feature space - there are
simply far less constraints on what shape the hyperplane needs to take on. In fact, if you were to take an infinite amount of features, the probability of a training sample laying on the wrong side of a hyperplane would become infinitely small and your classification performance on the training samples would be perfect.</p>
<p>In this way, the curse of dimensionality causes <strong>overfitting</strong> on the training set: the learned hyperplane does not reflect actual real-world differences between cats and dogs (the test set), but instead reflects the appearance of individual cats and dogs that just happen to appear in our training set. In this instance, a much better choice would be to do linear classification on two features: while performance on the training set would be lower, the model would <em>generalize</em> much better to the
new, unseen data of the test set.</p>
<p><p><img alt="803c52968c634a79ba46f533ca6a8519" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p><p>### 2.2 Basics of PCA After the cat/dog example, we are left with an obvious question: “if we shouldn’t use too many features due to the curse of dimensionality, which features <em>should</em> we use?” PCA is often used to determine which features are most suitable for a classification problem. We begin by illustrating the basic principles of PCA on a Gaussian dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">xlabel</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>

    <span class="c1"># If no pre-existing axis given, make new plot</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

    <span class="c1"># Plot and format figure</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">draw_vector</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">arrowprops</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span> <span class="o">=</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span>
                      <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                      <span class="n">shrinkA</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">shrinkB</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">arrowprops</span> <span class="o">=</span> <span class="n">arrowprops</span><span class="p">)</span> <span class="c1"># Draw vector from coords (start) to (end)</span>
</pre></div>
</div>
</div>
<div class="line-block">
<div class="line">Our dataset is defined as an <span class="math notranslate nohighlight">\(M\)</span>-by-2 matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> containing <span class="math notranslate nohighlight">\(M\)</span> points sampled from a two dimensional Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}\)</span> with <span class="math notranslate nohighlight">\(\mu_1 = 0, \mu_2 = 0\)</span></div>
<div class="line">and a covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma} = \begin{pmatrix} 2 &amp; 1\\ 1 &amp; 2 \end{pmatrix}\)</span>. Because of the covariance matrix, there is a positive skew upwards when plotting the data.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]],</span> <span class="n">n_samples</span><span class="p">)</span> <span class="c1"># Make 2D gaussian dataset</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;A skewed 2D Gaussian distribution&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature #1&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature #2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_8_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_8_0.png" />
</div>
</div>
<p>PCA works by finding the ‘principal components’ of an <span class="math notranslate nohighlight">\(N\)</span>-dimensional dataset (here <span class="math notranslate nohighlight">\(N\)</span> = 2). One reasonable way to think of principal components is that they are the directions in which the dataset shows the most variation, i.e. the largest spread in values. Typically, these directions of large variance are the interesting parts of the dataset: imagine a dataset with 100 features, 98 of which barely have any spread, and 2 of which show large amounts of variance. You can intuitively
imagine that these 2 features, because of their variance, must have some power to discriminate between classes.</p>
<p>Principal components are always orthogonal to one another and together form a new basis that we can use to transform the data. As a result of the transformation, the data will be linearly uncorrelated, meaning the covariance matrix will be diagonal: <span class="math notranslate nohighlight">\(\mathbf{\Sigma} = \begin{pmatrix} a &amp; 0\\ 0 &amp; b \end{pmatrix}\)</span>, with <span class="math notranslate nohighlight">\(\{a, b\} \geq 0\)</span>. This also means that the directions of greatest variances are now aligned with the axes: the first coordinate is now the first principal component,
the second coordinate is the second principal component, and so on.</p>
</section>
<section id="Mathematical-background">
<h2><em>Mathematical background</em><a class="headerlink" href="#Mathematical-background" title="Permalink to this headline">¶</a></h2>
<p>The process begins by centering the data <span class="math notranslate nohighlight">\(X\)</span> (<span class="math notranslate nohighlight">\(M\)</span> samples by 2 features) around the origin by subtracting the means of each variable from that column: <span class="math">\begin{equation}
\mathbf{\hat{X}} = \mathbf{X} - \mathbf{\bar{X}}
\end{equation}</span></p>
<p>We can find the principal components for the matrix <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span> by calculating its covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> and calculating the corresponding eigenvalues and eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>. The eigenvectors represent the principal components of the data and the eigenvalues represent the amount of variance explained by that principle component (proving that this is the case is outside of the scope of this course).</p>
<p>We can calculate the covariance matrix with the following formula: <span class="math">\begin{equation}
\mathbf{\Sigma} = \frac{1}{M-1}\mathbf{X^{T}}\mathbf{X}
\end{equation}</span></p>
<p>The easiest way to then calculate the eigenvectors and eigenvalues is to factorize <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> using singular value decomposition (SVD). SVD gives us the following expression: <span class="math">\begin{equation}
\mathbf{\Sigma} = \mathbf{U}\mathbf{s}\mathbf{V}
\end{equation}</span> where <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> contains the eigenvectors <span class="math notranslate nohighlight">\(\mathbf{u}^{(i)}\)</span> in the columns, ordered by largest to smallest variance:</p>
<p>:nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>begin{equation}
mathbf{U} =
left[</p>
<blockquote>
<div><dl>
<dt>begin{array}{cccc}</dt><dd><div class="line-block">
<div class="line">&amp; | &amp;        &amp; | \</div>
</div>
<p>mathbf{u}^{(1)} &amp; mathbf{u}^{(2)}    &amp; ldots &amp; mathbf{u}^{(n)} \
| &amp; | &amp;        &amp; |</p>
</dd>
</dl>
<p>end{array}</p>
</div></blockquote>
<p>right]
end{equation}` and <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> is a vector containing the eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span>: <span class="math">\begin{equation}
\mathbf{s} = \left[\lambda_1, \lambda_2, \ldots \lambda_n\right]
\end{equation}</span></p>
<p>Now, we can simply multiple <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> with our data <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span> to transform it to the new basis: <span class="math">\begin{equation}
\mathbf{X_{\text{PCA}}} = \mathbf{U^T}\mathbf{\hat{X}}
\end{equation}</span> In code, this looks like the following:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">pca_transform</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">X_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span> <span class="c1"># Center data</span>

    <span class="n">sigma_hat</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n_samples</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">X_hat</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_hat</span><span class="p">)</span> <span class="c1"># Calculate covariance matrix, alternative is np.cov(X)</span>

    <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">sigma_hat</span><span class="p">)</span> <span class="c1"># Do singular value decomposition to get eigen vector/values</span>

    <span class="n">X_pca</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_hat</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># Transform dataset using eigenvectors</span>

    <span class="k">return</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">s</span>

<span class="n">X_pca</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">pca_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We plot the original dataset and the PCA transformed dataset side by side. Superimposed on the plots, we show the principal components, before and after rotation. Note how the vectors point in the direction of the most variance.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># We plot vectors from (X_mean) - i.e. the data center - to (X _mean + sqrt(eigenvalue)*eigenvector)</span>
<span class="c1"># We use sqrt(eigenvalue) as a scaling factor to show the relative &quot;importance&quot; of the eigenvectors</span>
<span class="c1"># But note that this has no semantic meaning/significance!</span>

<span class="n">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;Principal components of the skewed Gaussian distribution&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature #1&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature #2&quot;</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">draw_vector</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span> <span class="n">X_mean</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">U</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">draw_vector</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span> <span class="n">X_mean</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">U</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">plot_data</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="s2">&quot;PCA transformed dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;Principal component #1&quot;</span><span class="p">,</span> <span class="s2">&quot;Principal component #2&quot;</span><span class="p">,</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">draw_vector</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span> <span class="n">X_mean</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">draw_vector</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span> <span class="n">X_mean</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]),</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_16_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_16_0.png" />
</div>
</div>
<p><p><img alt="13f56e2f0f354e3091c77eed0e05d4b5" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="2.3-Dimensionality-reduction-using-PCA">
<h2>2.3 Dimensionality reduction using PCA<a class="headerlink" href="#2.3-Dimensionality-reduction-using-PCA" title="Permalink to this headline">¶</a></h2>
<p>In the previous section, we showed the basic principles of PCA. However, dimensionality reduction is obviously not very important if you only have two features. PCA becomes much more useful for datasets where there are too many features to plot in a human understandable way. In that case, we can select only a subset of the first <span class="math notranslate nohighlight">\(n\)</span> eigenvectors of matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> (principal components) to do our transformation with. This means we project the data onto fewer axes and get a lower
dimensional dataset! For example, if we have a 100-dimensional dataset and we choose <span class="math notranslate nohighlight">\(n = 10\)</span>, we only take the first 10 columns of <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> and use this <span class="math notranslate nohighlight">\(\mathbf{U_{\text{reduced}}}\)</span> to project 100 dimensions onto 10 dimensions.</p>
<p>We usually decide on the number <span class="math notranslate nohighlight">\(n\)</span> as follows: we ask <em>how many principal components we need to retain :math:`geq` 95% of the dataset’s variance.</em> To reiterate: in dimensionality reduction, we care about retaining as much as the data’s variance as possible, while using as little dimensions as possible.</p>
<p>We can describe ‘retained variance’ with the eigenvalues of the principal components: for <span class="math notranslate nohighlight">\(n\)</span> vectors and a <span class="math notranslate nohighlight">\(k\)</span>-dimensional dataset, the retained variance <span class="math notranslate nohighlight">\(r\)</span> is: <span class="math">\begin{equation}
r = \sum_{i=1}^{n}\lambda_i \bigg/ \sum_{i=1}^{K}\lambda_i
\end{equation}</span></p>
<p>In other words, we divide the variance that the first <span class="math notranslate nohighlight">\(n\)</span> principal components retain by the total variance of all principal components. The variance of a principal component is represented by its eigenvalue.</p>
</section>
<section id="A-graphical-example:-cell-nuclei">
<h2><em>A graphical example: cell nuclei</em><a class="headerlink" href="#A-graphical-example:-cell-nuclei" title="Permalink to this headline">¶</a></h2>
<p>Let’s give an example using a high dimensional data set: we use the cell nuclei dataset from the CAD project.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">scipy.io</span> <span class="kn">import</span> <span class="n">loadmat</span>

<span class="k">def</span> <span class="nf">plot_series</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">stochastic</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span><span class="n">m</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n</span><span class="o">*</span><span class="n">m</span><span class="p">)</span> <span class="k">if</span> <span class="n">stochastic</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
    <span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">m</span><span class="p">),</span> <span class="n">ix</span><span class="p">):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[:,:,:,</span><span class="n">j</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>

    <span class="k">return</span> <span class="n">axs</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Load dataset</span>
<span class="n">fn</span> <span class="o">=</span> <span class="s1">&#39;../data/nuclei_data.mat&#39;</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">loadmat</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

<span class="n">images</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="s2">&quot;training_images&quot;</span><span class="p">]</span>       <span class="c1"># shape (24, 24, 3, 21910)</span>
<span class="n">images_y</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="s2">&quot;training_y&quot;</span><span class="p">]</span>          <span class="c1"># shape (21910, 1)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Visualize - randomized, so refresh to see new nuclei!</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">plot_series</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_20_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_20_0.png" />
</div>
</div>
<p>In the CAD project, you must perform linear regression on the raw pixel values of the images to predict the surface area of nuclei. Here, we don’t do regression, but purely investigate how many principal components we need to properly represent this high dimensional dataset, for the purposes of dimensional reduction. Because the images are of shape <span class="math notranslate nohighlight">\((24,24,3)\)</span>, we get a total of <span class="math notranslate nohighlight">\(24x24x3 = 1728\)</span> features per image. As our training set consists of 21910 images, we will have a
<span class="math notranslate nohighlight">\((21910,1728)\)</span> shaped dataset <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">shapes</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">shape</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">shapes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_features</span><span class="p">,</span> <span class="n">shapes</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The shape of our dataset X is: &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The shape of our dataset X is:  (21910, 1728)
</pre></div></div>
</div>
<p>Let’s perform PCA on our dataset and see how many principal components we need to retain 95% of the data variance.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X_pca</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">pca_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Every iteration i, divide variance retained in s[0:i] by total variance</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">r</span> <span class="o">&gt;</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The number of principal components needed to retain 95 percent data variance is: &quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The number of principal components needed to retain 95 percent data variance is:  132
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.95</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Number of principal components&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Retained variance (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">1728</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">,</span> <span class="s2">&quot;95 percent retained variance&quot;</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s2">&quot;bold&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Retained variance as a function of the number of principal components&quot;</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s1">&#39;bold&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_25_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_25_0.png" />
</div>
</div>
<p>Remarkably, we only need 132 principal components (dimensions) to retain 95 percent of our data variance, even for a 1728 dimensional dataset. We can now easily reduce the dimensionality of our dataset by transforming the data with the first 132 eigenvectors. In a real scenario, we could now use this transformed dataset to perform regression (and also classification) techniques on. In general, because of the lower dimensionality, techniques will be less likely to overfit.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">U_reduced</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">132</span><span class="p">]</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">U_reduced</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_hat</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># Our new, transformed (and lower dimensional) dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The new shape of our data matrix now is: &quot;</span><span class="p">,</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The new shape of our data matrix now is:  (21910, 132)
</pre></div></div>
</div>
<p><p><img alt="c985402deedd45adbd4ce5929254cacb" class="no-scaled-link" src="../../_images/read_ico.png" style="width: 42px; height: 42px;" /></p>
</p><p>### 2.4 Intuitive interpretations of principle components Lastly, we show two additional intuitive interpretations of principle components.</p>
<p>We have also established that principle components can be thought of the orthogonal directions of variation in our dataset. However, beyond just ‘direction’, we can also visualize this variation in the form of an image! Consider the fact that the eigenvectors are of the shape <span class="math notranslate nohighlight">\((1, 1728)\)</span> and can thus be reshaped back into the original shape of the image, <span class="math notranslate nohighlight">\((24,24,3)\)</span>. In this form, the eigenvectors represent the <em>principle modes of variation</em> in the image.</p>
<p>We visualize this with the code in the cell below. Per eigenvector, we reshape the vectors to images (and rescale the values to [0,255]). The first eigenvector (top left) is a disk, which makes sense: since our dataset depicts nuclei, typically round, centered objects, the most variation exists in that form. As we go further right and down, this variation becomes more abstract.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># From here, we use the sklearn PCA function, it&#39;s much better optimized than our own code.</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">cad_PCA</span> <span class="kn">import</span> <span class="n">reconstruction_demo</span><span class="p">,</span> <span class="n">reshape_and_rescale</span>

<span class="c1"># Rescaling is necessary because the eigenvectors are not in [0,255] domain</span>
<span class="n">reconstructed_ims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">reshape_and_rescale</span><span class="p">(</span><span class="n">U</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])],</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plot_series</span><span class="p">(</span><span class="n">reconstructed_ims</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">stochastic</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_29_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_29_0.png" />
</div>
</div>
<p>Second of all, we can use the eigenvectors in the matrix <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> to reconstruct our images from the transformed dataset <span class="math notranslate nohighlight">\(\mathbf{X}_{\text{PCA}}\)</span>. We simply multiply the data with <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> again, because:</p>
<p><span class="math">\begin{equation}
\mathbf{X_{\text{PCA}}} = \mathbf{U^T}\mathbf{\hat{X}} \\
\mathbf{X_{\text{rec}}} = \mathbf{U}\mathbf{X_{\text{PCA}}} = \mathbf{U}\mathbf{U^T}\mathbf{\hat{X}} = \mathbf{I}\mathbf{\hat{X}} \\
\end{equation}</span> Then we just have to add the mean back to the data (“uncentering” it), and we get back our original dataset <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. <span class="math">\begin{equation}
\mathbf{X} = \mathbf{X_{\text{rec}}} + \mathbf{\bar{X}}
\end{equation}</span></p>
<p>Just like with the eigenvectors, we can reshape the individual rows of the dataset <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> back into an image. However, we can also reconstruct and reshape the images with only a subset of the eigenvectors, so with <span class="math notranslate nohighlight">\(\mathbf{U_\text{reduced}}\)</span>. In the demo below, we show the resulting images for the first 200 eigenvectors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">reconstruction_demo</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pca</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_31_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_31_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_31_1.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_31_1.png" />
</div>
</div>
<p>As you can see, as the number of principle components used to reconstruct the images with increases, the quality of the images becomes better. This is because we able to utilize more information of the dataset in the reconstruction. In effect, we observe that each principle component is a linear combination of all features of our dataset.</p>
</section>
</section>
<section id="3.-Principal-component-analysis-(exercises)">
<h1>3. Principal component analysis (exercises)<a class="headerlink" href="#3.-Principal-component-analysis-(exercises)" title="Permalink to this headline">¶</a></h1>
<p><p><img alt="0fc0261079834645af726bf0a171c4bb" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p><section id="Exercise-3.1:">
<h2><em>Exercise 3.1</em>:<a class="headerlink" href="#Exercise-3.1:" title="Permalink to this headline">¶</a></h2>
<p>Use the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">generate_gaussian_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
<p>to generate a dataset with correlated features. Calculate the mean and covariance matrix of the data using <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">cov</span></code> and compare them to the parameters you used as input. Write your implementation in <code class="docutils literal notranslate"><span class="pre">covariance_matrix_test()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">2</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">covariance_matrix_test</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">covariance_matrix_test</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Mean:
[-0.20086499 -0.04288037]
Covariance matrix:
[[3.42013403 0.48687568]
 [0.48687568 0.59369576]]
</pre></div></div>
</div>
<p><p><img alt="6900a92aa0d443cc8e6f8e96daaaddc9" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-3.1:">
<h2><em>Question 3.1</em>:<a class="headerlink" href="#Question-3.1:" title="Permalink to this headline">¶</a></h2>
<p>Is there a difference? How could you increase or decrease this difference?</p>
<p>Type your answer here</p>
<p><p><img alt="c6c9c74210564808a21e0980326f467a" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-3.2:">
<h2><em>Exercise 3.2</em>:<a class="headerlink" href="#Exercise-3.2:" title="Permalink to this headline">¶</a></h2>
<p>Compute the eigenvectors and eigenvalues of the covariance matrix using:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
</pre></div>
</div>
<p>(the column <code class="docutils literal notranslate"><span class="pre">v[:,i]</span></code> is the eigenvector corresponding to the eigenvalue <code class="docutils literal notranslate"><span class="pre">w[i]</span></code>).</p>
<p>Inspect the eigenvectors and eigenvalues. What two properties can you name about the eigenvectors? How can you verify these properties (describe the operations, or give a line of Python code). For the eigenvalues, which eigenvalue is the largest and which is the smallest?</p>
<p>You can sort the eigenvalues and eigenvectors as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">w</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1">#Find ordering of eigenvalues</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="c1">#Reorder eigenvalues</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="n">ix</span><span class="p">]</span> <span class="c1">#Reorder eigenvectors</span>
</pre></div>
</div>
<p>Write your implementation in <code class="docutils literal notranslate"><span class="pre">eigen_vecval_test()</span></code> in <code class="docutils literal notranslate"><span class="pre">SECTION</span> <span class="pre">2</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">eigen_vecval_test</span>
<span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">eigen_vecval_test</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Eigenvalues w:
[4.07875811 0.64964702]
Eigenvectors v:
[[ 0.97908048 -0.2034734 ]
 [ 0.2034734   0.97908048]]
Order v:
[0 1]
Reordered eigenvalues v:
[4.07875811 0.64964702]
Reordered eigenvectors v:
[[ 0.97908048 -0.2034734 ]
 [ 0.2034734   0.97908048]]
Checking results:
Dot procuct of first eigenvector with itself:
1.0000000000000002
Dot procuct of the two eigenvectors:
0.0
</pre></div></div>
</div>
<p><p><img alt="4ca253cf4bb14a29a4a8ff3622e99d87" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-3.3:">
<h2><em>Exercise 3.3</em>:<a class="headerlink" href="#Exercise-3.3:" title="Permalink to this headline">¶</a></h2>
<p>Rotate the data using <code class="docutils literal notranslate"><span class="pre">v</span></code>. This is similar to what you did in the registration project, only now instead of getting the angle of rotation, <code class="docutils literal notranslate"><span class="pre">v</span></code> is already the rotation matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">rotate_using_eigenvectors_test</span>
<span class="n">X_rotated</span> <span class="o">=</span> <span class="n">rotate_using_eigenvectors_test</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_40_0.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_40_0.png" />
</div>
</div>
<p><p><img alt="b2ace76681be469dbe3dfd0325f2fb57" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-3.2:">
<h2><em>Question 3.2</em>:<a class="headerlink" href="#Question-3.2:" title="Permalink to this headline">¶</a></h2>
<p>In most literature you will see the notation <span class="math notranslate nohighlight">\(v^{T}*X\)</span>, but this will not work on our dataset because of how the dataset is defined (rows = samples, columns = dimensions). Instead use <span class="math notranslate nohighlight">\(X_{pca}=v^{T}*X^{T}\)</span> and <span class="math notranslate nohighlight">\(X_{pca}=X_{pca}^{T}\)</span>. What can you say about the covariance matrix of <code class="docutils literal notranslate"><span class="pre">Xpca</span></code>?</p>
<p>Type your answer here</p>
<p><p><img alt="e86a3144e5484583815af1923c2009dc" class="no-scaled-link" src="../../_images/todo_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Exercise-3.4:">
<h2><em>Exercise 3.4</em>:<a class="headerlink" href="#Exercise-3.4:" title="Permalink to this headline">¶</a></h2>
<p>Complete the missing functionality in the function <code class="docutils literal notranslate"><span class="pre">mypca()</span></code> in <code class="docutils literal notranslate"><span class="pre">#</span> <span class="pre">SECTION</span> <span class="pre">2</span></code> of the <code class="docutils literal notranslate"><span class="pre">segmentation.py</span></code> module. Test the function by running the <code class="docutils literal notranslate"><span class="pre">test_mypca()</span></code> script located in the <code class="docutils literal notranslate"><span class="pre">segmentation_tests.py</span></code> module. This will plot the original data, and the data after <code class="docutils literal notranslate"><span class="pre">test_mypca()</span></code> is applied. Here is how the result might look:</p>
<p><img alt="2fd55bf768cc491cba7a985629b4b5d0" class="no-scaled-link" src="reader/notebooks/assets/test_mypca.png" style="width: 800px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">segmentation_tests</span> <span class="kn">import</span> <span class="n">test_mypca</span>

<span class="n">test_mypca</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[0.90223878]
 [1.        ]]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_44_1.png" src="../../_images/reader_reader_2.6_CNNs_unsupervised_learning_PCA_44_1.png" />
</div>
</div>
<p><p><img alt="0fbc3ee4dd3b45358b0db260340a564b" class="no-scaled-link" src="../../_images/question_ico.png" style="width: 42px; height: 42px;" /></p>
</p></section>
<section id="Question-3.3:">
<h2><em>Question 3.3</em>:<a class="headerlink" href="#Question-3.3:" title="Permalink to this headline">¶</a></h2>
<p>You might have noticed when editing <code class="docutils literal notranslate"><span class="pre">mypca()</span></code> that there is an additional output, <code class="docutils literal notranslate"><span class="pre">fraction_variance</span></code>. This vector stores how much variance is accounted for by the first, first two, first three etc principal components. How much variance is the first principal component responsible for in the Gaussian data you just generated? How would you need to modify the covariance matrix of the data, in order to decrease the amount of variance in the first principal component? You can test your
hypothesis by modifying the properties of the Gaussian data created at the start of <code class="docutils literal notranslate"><span class="pre">test_mypca()</span></code>.</p>
<p>Note that not any matrix is a valid covariance matrix so if you just enter random numbers you are likely to get an error. To start, the matrix needs to be symmetric, and the diagonal values need to be positive. Furthermore, the covariance cannot be large if the variance is small. You can read about how to verify this here: <a class="reference external" href="https://math.stackexchange.com/questions/1522397/how-to-tell-is-a-matrix-is-a-covariance-matrix">https://math.stackexchange.com/questions/1522397/how-to-tell-is-a-matrix-is-a-covariance-matrix</a>.</p>
<p>Type your answer here</p>
</section>
</section>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Daniel Krahulec.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>