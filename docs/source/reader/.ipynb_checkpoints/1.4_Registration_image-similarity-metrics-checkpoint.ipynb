{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1.4: Image similarity metrics\n",
    "\n",
    "This notebook combines probability theory with exercises on image similarity metrics. Implement all functions in the `code` folder of your cloned repository, and test it in this notebook after implementation by importing your functions to this notebook. Use available markdown sections to fill in your answers to questions as you proceed through the notebook.\n",
    "\n",
    "**Contents:** <br>\n",
    "\n",
    "1. [Probability theory](#prob_theory)<br>\n",
    "2. [Image similarity metrics](#img_sim_metrics)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"prob_theory\"></div>\n",
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/read_ico.png\" width=\"42\" height=\"42\"></div> \n",
    "\n",
    "##  1. Probability theory\n",
    "\n",
    "### Random variables\n",
    "Random variables map the outcomes of random phenomena to numbers. Remember the example with coin tossing in the lecture? There, we had a random variable $X$ (outcome of the coin toss), and another random variable $Y$ (the number of heads in a series of 3 tosses). To represent possible values and the respective probabilities of the magnitude of a random variable, we use probability distribution functions. In a similar way, we can define medical image intensities as random variables.\n",
    "\n",
    "### Probability mass function (a.k.a probability distribution table)\n",
    "Random phenomenon: Pick a random pixel location. In this case, the pixel intensity can be treated as a random variable. Each outcome from the random phenomenon we are studying can be associated with a probability. If a random variable $X$ can have a finite set of possible values, we can define a function that maps each possible value to a probability. This function is called **probability mass function** (PMF), and expresses a *discrete probability distribution*. \n",
    "\n",
    "Probability mass function: \n",
    "\n",
    "$p_{X}(x) = P(X = x)$\n",
    "\n",
    "<center width=\"100%\"><img src=\"../reader/assets/pmf.png\" width=\"500\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Fong Chun Chan's Blog](https://tinyheero.github.io/2016/03/17/prob-distr.html)</font>\n",
    "\n",
    "What if we have two random variables? For example, the pixel intensity in two images. In such case, we can define a joint probability mass function:\n",
    "\n",
    "$p_{X,Y}(x,y) = P(X = x, Y = y)$\n",
    "\n",
    "PMF can be used to determine the probability of an observation being exactly equal to a discrete target value. But how can we define the probability mass function for the image intensities? We can use image histogram for this purpose by counting the number of occurrences of each intensity value in the image. In order to treat the counts of the histogram as probability values, we must normalize the histogram in such a way that all values sum to 1. This is the probability mass function for the pixel intensity as a random variable. \n",
    "\n",
    "### Probability density function\n",
    "Probability mass function is defined for discrete random variables. In case of continuous random variables, however, their probabilities are not directly measurable, and we therefore calculate the probability as the proportion of times. Imagine you had a random variable that measured the price of a diamond. Now, what is the probability that a single diamond's price is exactly equal to e.g. 150 USD? The probability of getting a diamond for that exact price would be very low, if any at all. Therefore, a given value of a variable on a continuous scale cannot be assigned a probability. We therefore need to think in terms of intervals instead of individual outcomes. For continuous random variables, which can take infinite number of possible values, we can define the **probability density function** (PDF), where the probability of $y \\in [a,b]$ is equivalent to the integral of the PDF between $a$ and $b$:\n",
    "\n",
    "$P(a \\leq y \\leq b) = \\int_{a}^{b}\\,f(Y)\\,dy$\n",
    "\n",
    "<center width=\"100%\"><img src=\"../reader/assets/pdf.png\" width=\"500\"></center>\n",
    "\n",
    "<font size=\"1\">Figure from [Fong Chun Chan's Blog](https://tinyheero.github.io/2016/03/17/prob-distr.html)</font>\n",
    "\n",
    "\n",
    "\n",
    "###  Bayes’ rule \n",
    "Bayes’ rule is a very useful formula that we will use later in the computer-aided diagnosis notebooks of this course. \n",
    "The so-called Bayes' theorem gives the probability of an event based on new information that is, or may be related, to that event. Mathematically, the Bayes' theorem can be expressed as follow: \n",
    "\n",
    "$p_{X|Y} = \\frac{p_{Y|X}(x|y)p_{Y}(y)}{p_{X}(x)}$   ,\n",
    "\n",
    "where $X$ and $Y$ are events and $P(Y) \\neq 0$, and:\n",
    "\n",
    "- $p_{X|Y}$ is the probability of event $X$ occurring given event $Y$ is true; also known as the posterior probability of $X$ given $Y$\n",
    "- $p_{Y|X}(x|y)$ is the likelihood of $X$ given a fixed $Y$\n",
    "- $p_{X}(x)$ and $p_{Y}(y)$ are the probabilities of observing the two events without any given conditions; also known as marginal or prior probabilities\n",
    "- $X$ and $Y$ are events (must not be the same)\n",
    "\n",
    "Bayes' theorem is typically utilized in diagnostic decision-making, e.g. to find out if there is a certain clinical manifestation in a patient before images are acquired. Given the prevalence of a disease, a radiologist is able to first estimate the marginal probability of the disease and afterwards assess medical images based on this prior. The Bayes' rule enables to derive positive predictive and negative predictive values in radiologists' pre-assessment tasks. Furthermore, this probability theorem also has its utility in cases with similar imaging findings in different diagnoses to calculate the probability at which certain imaging characteristics pertain to rare or common diagnoses (regardless of complete clinical contexts). The Bayes' theorem is also used in algorithms for medical image artefact corrections, such as in MRI and perfusion-weighted images to reduce noise. Bayesian inference has a wide range of applications in AI-driven radiology software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='img_sim_metrics'></div>\n",
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/read_ico.png\" width=\"42\" height=\"42\"></div> \n",
    "\n",
    "## 2. Image similarity metrics\n",
    "\n",
    "The concepts of image-based similarity metrics (sum of square differences, cross-correlation and mutual information) were covered in the previous notebook on [Intensity-based registration](../reader/1.3_Registration_intensity-based-registration.ipynb).\n",
    "\n",
    "In this section, you will implement two such image similarity metrics: correlation and mutual information. The computation of the mutual information between two images relies on their joint histogram, so one of the exercises deals with the implementation of this intermediate step. In the project work section you will use the similarity metrics to find the optimal rotation transformation that aligns two images.\n",
    "\n",
    "Before you start, load your favorite test image in Python. You're going to use this image to test your implementation. Some of the examples below assume you work with images of type `uint8`, i.e. pixel intensities in the $[0, 255]$ range, but the provided functions are equipped to work with arbitrary image types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Question 2.1:*\n",
    "\n",
    "How can we measure the quality of a registration task? Which consideration is important when selecting target points for which we measure registration performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Normalized cross-correlation\n",
    "The normalized cross-correlation between two images $I$ and $J$ with pixels $i$ and respective\n",
    "mean intensities $\\overline{I}$ and $\\overline{J}$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "C=\\frac{\\sum_{i=1}^{n}(I(i)-\\overline{I})(J(i)-\\overline{J})}{\\sqrt{\\sum_{i=1}^{n}(I(i)-\\overline{I})^{2} \\sum_{j=1}^{n}(J(i)-\\overline{J})^{2}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the number of image pixels. If we reshape the 2D images to vectors (in Python this can be done with `numpy.reshape()`), the expression for the normalized cross-correlation can be rewritten using vector multiplication operators (which will also make it more clear how to implement it in Python):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "C=\\frac{(\\mathbf{u}-\\overline{I})^{\\top}(\\mathbf{v}-\\overline{J})}{\\sqrt{(\\mathbf{u}-\\overline{I})^{\\mathrm{T}}(\\mathbf{u}-\\overline{I})} \\sqrt{(\\mathbf{v}-\\overline{J})^{\\mathrm{T}}(\\mathbf{v}-\\overline{J})}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{u}$ and $\\mathbf{v}$ are vectors of the pixels intensities of the images $I$ and $J$, respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "u=\\left[ \\begin{array}{c}{I(1)} \\\\ {I(2)} \\\\ {\\vdots} \\\\ {I(n)}\\end{array}\\right], v=\\left[ \\begin{array}{c}{J(1)} \\\\ {J(2)} \\\\ {\\vdots} \\\\ {J(n)}\\end{array}\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/todo_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Exercise 2.1.1:*\n",
    "\n",
    "The provided function `correlation()` in `SECTION 3` of the `registration.py` module contains a template for implementing the normalized cross-correlation metric. Most of the functionality such as parameter checking and pre-processing of the images is already implemented. The only piece of code that is missing is the computation of the normalized cross-correlation using the equation above.\n",
    "\n",
    "Implement the missing functionality in the `correlation()` function. Note that the mean intensity is already subtracted from the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/todo_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Exercise 2.1.2:*\n",
    "\n",
    "Test your implementation using the template `correlation_test()` script provided in `SECTION 3` of the `registration_tests.py` module. For example, you can make sure that `correlation(I,I)`, i.e. the normalized cross-correlation of any image with itself, returns 1. Use some other properties of normalized cross-correlation in order to further test your implementation. (**Tip**: How does a linear transformation of the intensities of the images affect the normalized cross-correlation coefficient?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from registration_tests import correlation_test\n",
    "\n",
    "#correlation_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Question 2.1.1:*\n",
    "\n",
    "Under which assumptions is the normalized cross-correlation the optimal similarity metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Joint histogram\n",
    "\n",
    "The `joint_histogram()` function in `SECTION 3` of the `registration.py` module contains an almost complete implementation of computation of the joint histogram of two images. We use the joint histogram as an estimate of the joint probability mass function (PMF) of two images. This function informs us of the probability that two intensities co-occur (appear together) at the same location in the two images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/todo_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Exercise 2.2.1:*\n",
    "\n",
    "Go over the implementation and make sure you understand the functionality of all the steps in the code. Implement the last missing step in the computation of the joint histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Question 2.2.1:*\n",
    "\n",
    "One of the parameters of the function is num bins, which defines the number of bins of the joint histogram. The default value for this parameter in this implementation is chosen to be 16. We mostly work with 8-bit images that have 256 possible values for the pixel intensities, which means that num bins can go as high as 256. However, there is a trade-off between choosing num bins too low or too high. What is this trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mutual Information\n",
    "\n",
    "Given the joint PMF of the two images $p_{I, J}$ and the two marginal PMF's (the PMF's of the individual images) $p_{I}$ and $p_{I}$, the mutual information can be computed according to the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "M I(I, J)=\\sum_{i=1}^{n} \\sum_{j=1}^{n} p_{I, J}(i, j) \\log \\frac{p_{I, J}(i, j)}{p_{I}(i) p_{J}(j)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the joint histogram is an estimate of the joint PMF. Thus, in the previous equation, we can \"plug in\" the joint histogram for $p_{I, J}$, and analogously, the marginal histograms (the histograms of the individual images) for $p_{I}$ and $p_{J}$. The two sum operators go over all bins in the joint histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/todo_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Exercise 2.3.1:*\n",
    "\n",
    "A template for implementation of mutual information given a joint histogram of two images is given in the Python function `mutual_information()` in `SECTION 3` of the `registration.py` module. As before, the file already contains all the pre-processing steps but the actual computation of the mutual information is missing. The only missing piece of code in the template file is implementation of the above formula for mutual information. Implement the missing functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/todo_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Exercise 2.3.2:*\n",
    "\n",
    "Use some of the properties of mutual information to test your implementation. Write these tests in the provided `mutual_information_test()` script in `SECTION 3` of the `registration_tests.py` module. (**Tip**: What would be the mutual information of two random noise images? You can generate random noise `uint8` images with `np.random.randint(255, size=(512, 512))`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from registration_tests import mutual_information_test\n",
    "\n",
    "#mutual_information_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/todo_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Exercise 2.3.3:*\n",
    "\n",
    "An alternative formula for mutual information is:\n",
    "\n",
    "$$\n",
    "MI(I,J)=H(I)+H(J)-H(I, J)\n",
    "$$\n",
    "\n",
    "In the previous equation, $H(I)$ and $H(J)$ is the entropy of the images $I$ and $J$ and $H(I,J)$ is their joint entropy. \n",
    "\n",
    "Find out the equation for the entropy and implement mutual information, using this formula, in the `mutual_information_e()` function in `SECTION 3` of the `registration.py` module. Test your implementation with the provided `mutual_information_e_test()` script. Make sure that both implementations output equal values (very small differences are possible due to rounding errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from registration_tests import mutual_information_e_test\n",
    "\n",
    "#mutual_information_e_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Question 2.3.1:*\n",
    "\n",
    "When is mutual information preferable over sum of squared errors and normalized cross-correlation as an image similarity metric? Motivate your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right;margin:-5px 5px\"><img src=\"../reader/assets/question_ico.png\" width=\"42\" height=\"42\"></div>  \n",
    "\n",
    "### *Question 2.3.2:*\n",
    "\n",
    "The output of `mutual_information()` is described as \"*mutual information in nat units*\". What change in the code would you have to make to output the mutual information in bits? Does it make a difference which units you output when you use the mutual information metric in practice (for example, to perform image registration)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Type your answer here</font>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "7cf3cfb4d2a53586223bf4603cd7f9e645cf44a77dbcec96182c9a81e54296ad"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
